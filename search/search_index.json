{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Gaussian-Dirichlet Random Fields \u00b6 Pytorch+GPytorch implementation of GDRFs from San Soucie et al. 2020 Free software: MIT Documentation: https://gdrf.readthedocs.io Features \u00b6 TODO Credits \u00b6 This package was created with Cookiecutter and the zillionare/cookiecutter-pypackage project template.","title":"home"},{"location":"#gaussian-dirichlet-random-fields","text":"Pytorch+GPytorch implementation of GDRFs from San Soucie et al. 2020 Free software: MIT Documentation: https://gdrf.readthedocs.io","title":"Gaussian-Dirichlet Random Fields"},{"location":"#features","text":"TODO","title":"Features"},{"location":"#credits","text":"This package was created with Cookiecutter and the zillionare/cookiecutter-pypackage project template.","title":"Credits"},{"location":"api/","text":"Top-level package for Gaussian-Dirichlet Random Fields. cli \u00b6 Console script for gdrf. infer special \u00b6 divergence \u00b6 NonlinearExpectationDivergence \u00b6 loss ( self , model , guide , * args , ** kwargs ) \u00b6 :returns: returns an estimate of the ELBO :rtype: float Evaluates the ELBO with an estimator that uses num_particles many samples/particles. Source code in gdrf/infer/divergence.py def loss ( self , model , guide , * args , ** kwargs ): \"\"\" :returns: returns an estimate of the ELBO :rtype: float Evaluates the ELBO with an estimator that uses num_particles many samples/particles. \"\"\" loss_particles = [] is_vectorized = self . vectorize_particles and self . num_particles > 1 # grab a vectorized trace from the generator for model_trace , guide_trace in self . _get_traces ( model , guide , args , kwargs ): loss_particle = self . loss_fn ( model_trace , guide_trace , args , kwargs ) loss_particles . append ( loss_particle ) if is_vectorized : loss_particles = loss_particles [ 0 ] else : loss_particles = torch . stack ( loss_particles ) loss = - loss_particles . sum () . item () / self . num_particles warn_if_nan ( loss , \"loss\" ) return loss loss_and_grads ( self , model , guide , * args , ** kwargs ) \u00b6 :returns: returns an estimate of the ELBO :rtype: float Computes the ELBO as well as the surrogate ELBO that is used to form the gradient estimator. Performs backward on the latter. Num_particle many samples are used to form the estimators. Source code in gdrf/infer/divergence.py def loss_and_grads ( self , model , guide , * args , ** kwargs ): \"\"\" :returns: returns an estimate of the ELBO :rtype: float Computes the ELBO as well as the surrogate ELBO that is used to form the gradient estimator. Performs backward on the latter. Num_particle many samples are used to form the estimators. \"\"\" loss_particles = [] surrogate_loss_particles = [] is_vectorized = self . vectorize_particles and self . num_particles > 1 tensor_holder = None # grab a vectorized trace from the generator for model_trace , guide_trace in self . _get_traces ( model , guide , args , kwargs ): surrogate_loss_particle = self . loss_fn ( model_trace , guide_trace , args , kwargs ) loss_particle = surrogate_loss_particle . detach () . item () if is_identically_zero ( loss_particle ): if tensor_holder is not None : loss_particle = torch . zeros_like ( tensor_holder ) surrogate_loss_particle = torch . zeros_like ( tensor_holder ) else : # loss_particle is not None if tensor_holder is None : tensor_holder = torch . zeros_like ( loss_particle ) # change types of previous `loss_particle`s for i in range ( len ( loss_particles )): loss_particles [ i ] = torch . zeros_like ( tensor_holder ) surrogate_loss_particles [ i ] = torch . zeros_like ( tensor_holder ) loss_particles . append ( loss_particle ) surrogate_loss_particles . append ( surrogate_loss_particle ) if tensor_holder is None : return 0.0 if is_vectorized : loss_particles = loss_particles [ 0 ] surrogate_loss_particles = surrogate_loss_particles [ 0 ] else : loss_particles = torch . stack ( loss_particles ) surrogate_loss_particles = torch . stack ( surrogate_loss_particles ) divergence = self . nonlinearity ( loss_particles . sum ( dim = 0 , keepdim = True ) / self . num_particles ) # collect parameters to train from model and guide trainable_params = any ( site [ \"type\" ] == \"param\" for trace in ( model_trace , guide_trace ) for site in trace . nodes . values () ) if trainable_params and getattr ( surrogate_loss_particles , \"requires_grad\" , False ): surrogate_divergence = - self . nonlinearity ( surrogate_loss_particles . sum () / self . num_particles ) surrogate_divergence . backward () loss = - divergence warn_if_nan ( loss , \"loss\" ) return loss loss \u00b6 Loss \u00b6 loss ( self , model , guide , * args , ** kwargs ) \u00b6 :returns: returns an estimate of the ELBO :rtype: float Evaluates the ELBO with an estimator that uses num_particles many samples/particles. Source code in gdrf/infer/loss.py @torch . no_grad () def loss ( self , model , guide , * args , ** kwargs ): \"\"\" :returns: returns an estimate of the ELBO :rtype: float Evaluates the ELBO with an estimator that uses num_particles many samples/particles. \"\"\" loss_particles = [] is_vectorized = self . vectorize_particles and self . num_particles > 1 # grab a vectorized trace from the generator for model_trace , guide_trace in self . _get_traces ( model , guide , args , kwargs ): loss_particle = self . loss_fn ( model_trace , guide_trace , args , kwargs ) loss_particles . append ( loss_particle ) if is_vectorized : loss_particles = loss_particles [ 0 ] else : loss_particles = torch . stack ( loss_particles ) loss = - loss_particles . sum () . item () / self . num_particles warn_if_nan ( loss , \"loss\" ) return loss loss_and_grads ( self , model , guide , * args , ** kwargs ) \u00b6 :returns: returns an estimate of the ELBO :rtype: float Computes the ELBO as well as the surrogate ELBO that is used to form the gradient estimator. Performs backward on the latter. Num_particle many samples are used to form the estimators. Source code in gdrf/infer/loss.py def loss_and_grads ( self , model , guide , * args , ** kwargs ): \"\"\" :returns: returns an estimate of the ELBO :rtype: float Computes the ELBO as well as the surrogate ELBO that is used to form the gradient estimator. Performs backward on the latter. Num_particle many samples are used to form the estimators. \"\"\" loss_particles = [] surrogate_loss_particles = [] is_vectorized = self . vectorize_particles and self . num_particles > 1 tensor_holder = None # grab a vectorized trace from the generator for model_trace , guide_trace in self . _get_traces ( model , guide , args , kwargs ): surrogate_loss_particle = self . loss_fn ( model_trace , guide_trace , args , kwargs ) loss_particle = surrogate_loss_particle . detach () . item () if is_identically_zero ( loss_particle ): if tensor_holder is not None : loss_particle = torch . zeros_like ( tensor_holder ) surrogate_loss_particle = torch . zeros_like ( tensor_holder ) else : # loss_particle is not None if tensor_holder is None : tensor_holder = torch . zeros_like ( loss_particle ) # change types of previous `loss_particle`s for i in range ( len ( loss_particles )): loss_particles [ i ] = torch . zeros_like ( tensor_holder ) surrogate_loss_particles [ i ] = torch . zeros_like ( tensor_holder ) loss_particles . append ( loss_particle ) surrogate_loss_particles . append ( surrogate_loss_particle ) if tensor_holder is None : return 0.0 if is_vectorized : loss_particles = loss_particles [ 0 ] surrogate_loss_particles = surrogate_loss_particles [ 0 ] else : loss_particles = torch . stack ( loss_particles ) surrogate_loss_particles = torch . stack ( surrogate_loss_particles ) loss_val = loss_particles . sum ( dim = 0 , keepdim = True ) / self . num_particles # collect parameters to train from model and guide trainable_params = any ( site [ \"type\" ] == \"param\" for trace in ( model_trace , guide_trace ) for site in trace . nodes . values () ) if trainable_params and getattr ( surrogate_loss_particles , \"requires_grad\" , False ): surrogate_loss_val = - surrogate_loss_particles . sum () / self . num_particles surrogate_loss_val . backward () loss = - loss_val warn_if_nan ( loss , \"loss\" ) return loss models special \u00b6 gdrf \u00b6 GDRF \u00b6 forward ( self , Xnew , full_cov = False ) \u00b6 Computes the mean and covariance matrix (or variance) of Gaussian Process posterior on a test input data :math: X_{new} : .. math:: p(f^* \\mid X_{new}, X, y, k, f_{loc}, f_{scale_tril}) = \\mathcal{N}(loc, cov). .. note:: Variational parameters f_loc , f_scale_tril , together with kernel's parameters have been learned from a training procedure (MCMC or SVI). :param torch.Tensor Xnew: A input data for testing. Note that Xnew.shape[1:] must be the same as self.X.shape[1:] . :param bool full_cov: A flag to decide if we want to predict full covariance matrix or just variance. :returns: loc and covariance matrix (or variance) of :math: p(f^*(X_{new})) :rtype: tuple(torch.Tensor, torch.Tensor) Source code in gdrf/models/gdrf.py @nn . pyro_method @scale_decorator ( \"Xnew\" ) def forward ( self , Xnew , full_cov = False ): r \"\"\" Computes the mean and covariance matrix (or variance) of Gaussian Process posterior on a test input data :math:`X_{new}`: .. math:: p(f^* \\mid X_{new}, X, y, k, f_{loc}, f_{scale\\_tril}) = \\mathcal{N}(loc, cov). .. note:: Variational parameters ``f_loc``, ``f_scale_tril``, together with kernel's parameters have been learned from a training procedure (MCMC or SVI). :param torch.Tensor Xnew: A input data for testing. Note that ``Xnew.shape[1:]`` must be the same as ``self.X.shape[1:]``. :param bool full_cov: A flag to decide if we want to predict full covariance matrix or just variance. :returns: loc and covariance matrix (or variance) of :math:`p(f^*(X_{new}))` :rtype: tuple(torch.Tensor, torch.Tensor) \"\"\" self . _check_Xnew_shape ( Xnew ) self . set_mode ( \"guide\" ) loc , cov = gp . util . conditional ( Xnew , self . scale ( self . xs ), self . _kernel , self . f_loc , self . f_scale_tril , full_cov = full_cov , whiten = self . _whiten , jitter = self . _jitter , ) return loc + self . _mean_function ( Xnew ), cov sparse_gdrf \u00b6 SparseGDRF \u00b6 forward ( self , Xnew , full_cov = False ) \u00b6 Computes the mean and covariance matrix (or variance) of Gaussian Process posterior on a test input data :math: X_{new} : .. math:: p(f^* \\mid X_{new}, X, y, k, f_{loc}, f_{scale_tril}) = \\mathcal{N}(loc, cov). .. note:: Variational parameters f_loc , f_scale_tril , together with kernel's parameters have been learned from a training procedure (MCMC or SVI). :param torch.Tensor Xnew: A input data for testing. Note that Xnew.shape[1:] must be the same as self.X.shape[1:] . :param bool full_cov: A flag to decide if we want to predict full covariance matrix or just variance. :returns: loc and covariance matrix (or variance) of :math: p(f^*(X_{new})) :rtype: tuple(torch.Tensor, torch.Tensor) Source code in gdrf/models/sparse_gdrf.py @scale_decorator ( \"Xnew\" ) def forward ( self , Xnew , full_cov = False ): r \"\"\" Computes the mean and covariance matrix (or variance) of Gaussian Process posterior on a test input data :math:`X_{new}`: .. math:: p(f^* \\mid X_{new}, X, y, k, f_{loc}, f_{scale\\_tril}) = \\mathcal{N}(loc, cov). .. note:: Variational parameters ``f_loc``, ``f_scale_tril``, together with kernel's parameters have been learned from a training procedure (MCMC or SVI). :param torch.Tensor Xnew: A input data for testing. Note that ``Xnew.shape[1:]`` must be the same as ``self.X.shape[1:]``. :param bool full_cov: A flag to decide if we want to predict full covariance matrix or just variance. :returns: loc and covariance matrix (or variance) of :math:`p(f^*(X_{new}))` :rtype: tuple(torch.Tensor, torch.Tensor) \"\"\" self . set_mode ( \"guide\" ) posterior_kernel = self . _kernel posterior_u_loc = self . u_loc posterior_u_scale_tril = self . u_scale_tril Luu = jittercholesky ( posterior_kernel ( torch . Tensor ( self . _inducing_points )) . contiguous (), self . M , self . jitter , self . maxjitter , ) f_loc , f_cov = gp . util . conditional ( Xnew , torch . Tensor ( self . _inducing_points ), posterior_kernel , torch . Tensor ( posterior_u_loc ), torch . Tensor ( posterior_u_scale_tril ), Luu , full_cov = full_cov , whiten = self . _whiten , jitter = self . _jitter , ) return f_loc + self . _mean_function ( Xnew ), f_cov topic_model \u00b6 CategoricalModel \u00b6 forward ( self , input ) \u00b6 Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in gdrf/models/topic_model.py def forward ( self , input : torch . Tensor ) -> torch . Tensor : self . set_mode ( \"guide\" ) return self . word_probs ( indices = input ) SpatioTemporalTopicModel \u00b6 forward ( self , input ) \u00b6 Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in gdrf/models/topic_model.py @abstractmethod def forward ( self , input : torch . Tensor ) -> torch . Tensor : pass utils \u00b6 generate_data_2d_circles ( K = 5 , V = 100 , N = 8 , W = 150 , H = 32 , R = 8 , eta = 0.1 , seed = 777 , device = 'cpu' , permute = False , constant_background = True ) \u00b6 This function generates an artifical 2D dataset with K total topics and V total words. Over a background topic with a uniform distribution of words, one-topic circles are scattered. Each non-background topic has a word distribution a la the Girdhar thesis word distributions. At each pixel, a random number of observations, uniformly distributed between V and 10V , is generated. Source code in gdrf/models/utils.py def generate_data_2d_circles ( K = 5 , V = 100 , N = 8 , W = 150 , H = 32 , R = 8 , eta = 0.1 , seed = 777 , device = \"cpu\" , permute = False , constant_background = True , ): \"\"\" This function generates an artifical 2D dataset with `K` total topics and `V` total words. Over a background topic with a uniform distribution of words, one-topic circles are scattered. Each non-background topic has a word distribution a la the Girdhar thesis word distributions. At each pixel, a random number of observations, uniformly distributed between `V` and `10V`, is generated. \"\"\" K_obj = K - 1 gen = torch . Generator ( device = device ) . manual_seed ( seed ) gen . manual_seed ( seed ) topics = torch . zeros (( W , H ), dtype = torch . int , device = device ) centers_x = torch . randint ( R , W - R , [ N ], device = device , generator = gen ) centers_y = torch . randint ( R , H - R , [ N ], device = device , generator = gen ) centers = torch . stack ([ centers_x , centers_y ]) . T obj_topics = torch . randint ( 0 , K_obj , [ N ], device = device , generator = gen ) xs = torch . reshape ( torch . stack ( torch . meshgrid ( torch . tensor ( list ( range ( W )), device = device ), torch . tensor ( list ( range ( H )), device = device ), ) ), ( 2 , W * H ), ) . T counts = torch . randint ( low = V , high = 10 * V , size = ( W * H ,), device = device , generator = gen ) while len ( obj_topics . unique ()) < K_obj and N >= K : obj_topics = torch . randint ( 0 , K_obj , [ N ], device = device , generator = gen ) obj_topics += 1 K_obj_probs = K_obj if constant_background else K p_v_z = torch . tensor ( [ [ 1 + eta if V * k / K_obj_probs <= v < V * ( k + 1 ) / K_obj_probs else eta for v in range ( V ) ] for k in range ( K_obj ) ], device = device , ) words = torch . zeros (( W * H , V ), dtype = torch . int , device = device ) probs = words * 0.0 if constant_background : bg_dist = torch . Tensor ([ 1 / V for _ in range ( V )]) . to ( probs . device ) else : bg_dist = torch . Tensor ( [ 1 + eta if V * ( K - 1 ) / K <= v <= V else eta for v in range ( V )] ) . to ( probs . device ) p_v_z = torch . cat ([ p_v_z , bg_dist . unsqueeze ( 0 )], dim = 0 ) p_v_z /= p_v_z . sum ( dim =- 1 , keepdim = True ) # p_v_z.shape = (K, V)] if permute : idx = torch . randperm ( p_v_z . shape [ 1 ]) p_v_z = p_v_z [:, idx ] . view ( p_v_z . size ()) probs = probs + p_v_z [ - 1 , :] for idx in range ( W * H ): w , h = xs [ idx , :] for i in range ( centers . shape [ 0 ]): diff = xs [ idx , :] - centers [ i , :] if ( diff * diff ) . sum () <= R * R : topics [ w , h ] = obj_topics [ i ] probs [ idx , :] = p_v_z [ obj_topics [ i ] - 1 , :] break words = torch . zeros (( W * H , V ), dtype = torch . int , device = device ) for idx in range ( len ( counts )): cats , obs = torch . multinomial ( input = probs [ idx , :], num_samples = counts [ idx ], replacement = True , generator = gen , ) . unique ( return_counts = True ) for i , cat in enumerate ( cats ): words [ idx , cat ] = obs [ i ] return centers , topics , p_v_z , xs , words generate_data_2d_uniform ( V = 100 , W = 150 , H = 32 , seed = 777 , device = 'cpu' ) \u00b6 This function generates an artifical 2D dataset with V total words. Word distributions are completely uniform everywhere. At each pixel, a random number of observations, uniformly distributed between V and 10V , is generated. Source code in gdrf/models/utils.py def generate_data_2d_uniform ( V = 100 , W = 150 , H = 32 , seed = 777 , device = \"cpu\" ): \"\"\" This function generates an artifical 2D dataset with `V` total words. Word distributions are completely uniform everywhere. At each pixel, a random number of observations, uniformly distributed between `V` and `10V`, is generated. \"\"\" gen = torch . Generator ( device = device ) . manual_seed ( seed ) gen . manual_seed ( seed ) xs = torch . reshape ( torch . stack ( torch . meshgrid ( torch . tensor ( list ( range ( W )), device = device ), torch . tensor ( list ( range ( H )), device = device ), ) ), ( 2 , W * H ), ) . T counts = torch . randint ( low = V , high = 10 * V , size = ( W * H ,), device = device , generator = gen ) words = torch . zeros (( W * H , V ), dtype = torch . int , device = device ) probs = words + 1.0 / V for idx in range ( len ( counts )): cats , obs = torch . multinomial ( input = probs [ idx , :], num_samples = counts [ idx ], replacement = True , generator = gen , ) . unique ( return_counts = True ) for i , cat in enumerate ( cats ): words [ idx , cat ] = obs [ i ] return xs , words visualize \u00b6 png_bytes_to_numpy ( png ) \u00b6 Convert png bytes to numpy array Source: https://gist.github.com/eric-czech/fea266e546efac0e704d99837a52b35f Credit: Eric Czech Source code in gdrf/models/visualize.py def png_bytes_to_numpy ( png ): \"\"\"Convert png bytes to numpy array Source: https://gist.github.com/eric-czech/fea266e546efac0e704d99837a52b35f Credit: Eric Czech \"\"\" return np . array ( Image . open ( BytesIO ( png ))) train \u00b6 Train a GDRF model on a custom dataset Usage $ python path/to/train.py --data mvco.csv train ( cfg = 'data/cfg.yaml' , project = 'wandb/mvco' , name = 'mvco_adamax_grid' , device = 'cuda:0' , exist_ok = True , weights = '' , data = 'data/data.csv' , dimensions = 1 , epochs = 3000 , resume = False , nosave = False , entity = None , upload_dataset = False , save_period =- 1 , artifact_alias = 'latest' , patience = 100 , verbose = True ) \u00b6 Trains a GDRF :param Union[str, dict] cfg: Config file or dict for training run with model and training hyperparameters :param str project: Project name for training run :param str name: Run name for training run :param str device: Device to store tensors on during training (e.g. 'cpu' or 'cuda:0') :param bool exist_ok: Existing project/name ok, do not increment :param str weights: Weights.pt file, if starting from pretrained weights :param str data: Data.csv file, with first row as column names and first dimensions columns as indexes :param int dimensions: Number of non-stationary index dimensions (e.g. a time series is 1D, x-y data are 2D, etc.) :param int epochs: Number of training epochs :param Union[str, bool] resume: Resume most recent training :param bool nosave: only save final checkpoint :param str entity: W&B entity :param bool upload_dataset: Upload dataset to W&B :param int save_period: Log model after this many epochs :param str artifact_alias: version of dataset artifact to be used :param int patience: EarlyStopping Patience (number of epochs without improvement before early stopping) :param bool verbose: Verbose Source code in gdrf/train.py def train ( # noqa: C901 cfg : Union [ str , dict ] = \"data/cfg.yaml\" , project : str = \"wandb/mvco\" , name : str = \"mvco_adamax_grid\" , device : str = \"cuda:0\" , exist_ok : bool = True , weights : str = \"\" , data : str = \"data/data.csv\" , dimensions : int = 1 , epochs : int = 3000 , resume : Union [ str , bool ] = False , nosave : bool = False , entity : str = None , upload_dataset : bool = False , save_period : int = - 1 , artifact_alias : str = \"latest\" , patience : int = 100 , verbose : bool = True , ): \"\"\" Trains a GDRF :param Union[str, dict] cfg: Config file or dict for training run with model and training hyperparameters :param str project: Project name for training run :param str name: Run name for training run :param str device: Device to store tensors on during training (e.g. 'cpu' or 'cuda:0') :param bool exist_ok: Existing project/name ok, do not increment :param str weights: Weights.pt file, if starting from pretrained weights :param str data: Data.csv file, with first row as column names and first ``dimensions`` columns as indexes :param int dimensions: Number of non-stationary index dimensions (e.g. a time series is 1D, x-y data are 2D, etc.) :param int epochs: Number of training epochs :param Union[str, bool] resume: Resume most recent training :param bool nosave: only save final checkpoint :param str entity: W&B entity :param bool upload_dataset: Upload dataset to W&B :param int save_period: Log model after this many epochs :param str artifact_alias: version of dataset artifact to be used :param int patience: EarlyStopping Patience (number of epochs without improvement before early stopping) :param bool verbose: Verbose \"\"\" opt = locals () callbacks = Callbacks () save_dir = Path ( str ( increment_path ( Path ( project ) / name , exist_ok = exist_ok ))) set_logging ( verbose = verbose ) print ( colorstr ( \"train: \" ) + \", \" . join ( f \" { k } = { v } \" for k , v in opt . items ())) check_git_status () check_requirements ( requirements = FILE . parent / \"requirements.txt\" , exclude = []) # Resume if resume and not check_wandb_resume ( resume ): # resume an interrupted run ckpt = ( resume if isinstance ( resume , str ) else get_latest_run () ) # specified or most recent path assert os . path . isfile ( ckpt ), \"ERROR: --resume checkpoint does not exist\" with open ( Path ( ckpt ) . parent . parent / \"opt.yaml\" ) as f : assert os . path . isfile ( Path ( ckpt ) . parent . parent / \"opt.yaml\" ) opt = yaml . safe_load ( f ) # replace with open ( Path ( ckpt ) . parent . parent / \"cfg.yaml\" ) as f : assert os . path . isfile ( Path ( ckpt ) . parent . parent / \"cfg.yaml\" ) cfg = yaml . safe_load ( f ) # replace opt [ \"cfg\" ], opt [ \"weights\" ], opt [ \"resume\" ] = cfg , ckpt , True weights = ckpt resume = True LOGGER . info ( f \"Resuming training from { ckpt } \" ) else : data = check_file ( data ) cfg = check_file ( cfg ) if isinstance ( cfg , str ): with open ( cfg ) as f : cfg = yaml . safe_load ( f ) # load hyps dict model_type = cfg [ \"model\" ][ \"type\" ] model_hyp = cfg [ \"model\" ][ \"hyperparameters\" ] kernel_type = cfg [ \"kernel\" ][ \"type\" ] kernel_hyp = cfg [ \"kernel\" ][ \"hyperparameters\" ] optimizer_type = cfg [ \"optimizer\" ][ \"type\" ] optimizer_hyp = cfg [ \"optimizer\" ][ \"hyperparameters\" ] objective_type = cfg [ \"objective\" ][ \"type\" ] objective_hyp = cfg [ \"objective\" ][ \"hyperparameters\" ] device = select_device ( device ) pretrained = weights . endswith ( \".pt\" ) # Directories w = save_dir / \"weights\" # weights dir w . mkdir ( parents = True , exist_ok = True ) # make dir last , best = w / \"last.pt\" , w / \"best.pt\" # Hyperparameters LOGGER . info ( colorstr ( \"config: \" ) + \", \" . join ( f \" { k } = { v } \" for k , v in cfg . items ())) # Save run settings with open ( save_dir / \"cfg.yaml\" , \"w\" ) as f : yaml . safe_dump ( cfg , f , sort_keys = False ) with open ( save_dir / \"opt.yaml\" , \"w\" ) as f : yaml . safe_dump ( opt , f , sort_keys = False ) data_dict = None # Loggers loggers = Loggers ( save_dir , weights , opt , cfg , LOGGER ) # loggers instance if loggers . wandb : data_dict = loggers . wandb . data_dict if resume : epochs , hyp = epochs , model_hyp # Register actions for k in methods ( loggers ): callbacks . register_action ( k , callback = getattr ( loggers , k )) # Config cuda = device != \"cpu\" init_seeds ( 1 ) data_dict = data_dict or check_dataset ( data ) # check if None train_path = data_dict [ \"train\" ] # Dataset dataset = ( pd . read_csv ( filepath_or_buffer = data , index_col = list ( range ( dimensions )), header = 0 , parse_dates = True , ) . fillna ( 0 ) . astype ( int ) ) index = dataset . index index = index . values if dimensions == 1 else np . array ( index . to_list ()) index = index - index . min ( axis =- dimensions , keepdims = True ) index = index / index . max ( axis =- dimensions , keepdims = True ) xs = torch . from_numpy ( index ) . float () . to ( device ) if dimensions == 1 : xs = xs . unsqueeze ( - 1 ) ws = torch . from_numpy ( dataset . values ) . int () . to ( device ) min_xs = xs . min ( dim = 0 ) . values . detach () . cpu () . numpy () . tolist () max_xs = xs . max ( dim = 0 ) . values . detach () . cpu () . numpy () . tolist () world = list ( zip ( min_xs , max_xs )) num_observation_categories = len ( dataset . columns ) # Kernel for k , v in kernel_hyp . items (): if isinstance ( v , float ): kernel_hyp [ k ] = torch . tensor ( v ) . to ( device ) kernel = KERNEL_DICT [ kernel_type ]( input_dim = dimensions , ** kernel_hyp ) kernel = kernel . to ( device ) # Model model = GDRF_MODEL_DICT [ model_type ]( xs = xs , ws = ws , world = world , kernel = kernel , num_observation_categories = num_observation_categories , device = device , ** model_hyp , ) # Optimizer optimizer = OPTIMIZER_DICT [ optimizer_type ]( optim_args = optimizer_hyp ) # Variational Objective objective = OBJECTIVE_DICT [ objective_type ]( vectorize_particles = True , ** objective_hyp ) start_epoch , best_fitness = 0 , float ( \"-inf\" ) if pretrained : ckpt = torch . load ( weights , map_location = device ) # load checkpoint exclude = [] # exclude keys csd = ckpt [ \"model\" ] . float () . state_dict () # checkpoint state_dict as FP32 csd = intersect_dicts ( csd , model . state_dict (), exclude = exclude ) # intersect model . load_state_dict ( csd , strict = False ) # load LOGGER . info ( f \"Transferred { len ( csd ) } / { len ( model . state_dict ()) } items from { weights } \" ) if ckpt [ \"optimizer\" ] is not None : optimizer . set_state ( ckpt [ \"optimizer\" ]) best_fitness = ckpt [ \"best_fitness\" ] # Epochs start_epoch = ckpt [ \"epoch\" ] + 1 if resume : assert ( start_epoch > 0 ), f \" { weights } training to { epochs } epochs is finished, nothing to resume.\" if epochs < start_epoch : LOGGER . info ( f \" { weights } has been trained for { ckpt [ 'epoch' ] } epochs. Fine-tuning for { epochs } more epochs.\" ) epochs += ckpt [ \"epoch\" ] # finetune additional epochs del ckpt , csd # SVI object scale = pyro . poutine . scale ( scale = 1.0 / len ( xs )) svi = pyro . infer . SVI ( model = scale ( model . model ), guide = scale ( model . guide ), optim = optimizer , loss = objective , ) LOGGER . info ( f \" { colorstr ( 'model:' ) } { type ( model ) . __name__ } \" ) LOGGER . info ( f \" { colorstr ( 'optimizer:' ) } { type ( optimizer ) . __name__ } \" ) LOGGER . info ( f \" { colorstr ( 'objective:' ) } { type ( objective ) . __name__ } \" ) # Resume callbacks . run ( \"on_pretrain_routine_end\" ) # Model parameters # Start training t0 = time . time () stopper = EarlyStopping ( best_fitness = best_fitness , patience = patience ) LOGGER . info ( f \"Logging results to { colorstr ( 'bold' , save_dir ) } \\n \" f \"Starting training for { epochs } epochs...\" ) with logging_redirect_tqdm (): pbar = trange ( start_epoch , epochs , initial = start_epoch , total = epochs ) for ( epoch ) in ( pbar ): # epoch ------------------------------------------------------------------ model . train () loss = svi . step ( xs , ws , subsample = False ) model . eval () perplexity = model . perplexity ( xs , ws ) . item () pbar . set_description ( f \"Epoch { epoch + 1 } \" ) pbar . set_postfix ( loss = loss , perplexity = perplexity ) callbacks . run ( \"on_train_epoch_end\" , epoch = epoch ) log_vals = [ loss , perplexity , model . kernel_lengthscale , model . kernel_variance , ] fi = - perplexity if fi > best_fitness : best_fitness = fi callbacks . run ( \"on_fit_epoch_end\" , log_vals , epoch , best_fitness , fi ) final_epoch = ( epoch + 1 == epochs ) or stopper . possible_stop if ( not nosave ) or final_epoch : ckpt = { \"epoch\" : epoch , \"best_fitness\" : best_fitness , \"model\" : deepcopy ( model ) . half (), \"optimizer\" : optimizer . get_state (), \"wandb_id\" : loggers . wandb . wandb_run . id if loggers . wandb else None , } torch . save ( ckpt , last ) if best_fitness == fi : torch . save ( ckpt , best ) del ckpt callbacks . run ( \"on_model_save\" , last , epoch , final_epoch , best_fitness , fi ) if stopper ( epoch = epoch - start_epoch , fitness = fi ): break # end epoch --------------------------------------------------------------------------------------- LOGGER . info ( f \" \\n { epoch - start_epoch + 1 } epochs completed in { ( time . time () - t0 ) / 3600 : .3f } hours.\" ) for f in last , best : if f . exists (): strip_optimizer ( f ) # strip optimizers callbacks . run ( \"on_train_end\" , last , best , xs , ws , dataset . index , dataset . columns , epoch ) LOGGER . info ( f \"Results saved to { colorstr ( 'bold' , save_dir ) } \" ) torch . cuda . empty_cache () return None utils special \u00b6 callbacks \u00b6 Callbacks \u00b6 \" Handles all registered callbacks for model hooks Adapted from YOLOv5, https://github.com/ultralytics/yolov5/ get_registered_actions ( self , hook = None ) \u00b6 \" Returns all the registered actions by callback hook Source code in gdrf/utils/callbacks.py def get_registered_actions ( self , hook = None ): \"\"\" \" Returns all the registered actions by callback hook Args: hook The name of the hook to check, defaults to all \"\"\" if hook : return self . _callbacks [ hook ] else : return self . _callbacks register_action ( self , hook , name = '' , callback = None ) \u00b6 Register a new action to a callback hook Source code in gdrf/utils/callbacks.py def register_action ( self , hook , name = \"\" , callback = None ): \"\"\" Register a new action to a callback hook Args: hook The callback hook name to register the action to name The name of the action for later reference callback The callback to fire \"\"\" assert ( hook in self . _callbacks ), f \"hook ' { hook } ' not found in callbacks { self . _callbacks } \" assert callable ( callback ), f \"callback ' { callback } ' is not callable\" self . _callbacks [ hook ] . append ({ \"name\" : name , \"callback\" : callback }) run ( self , hook , * args , ** kwargs ) \u00b6 Loop through the registered actions and fire all callbacks Source code in gdrf/utils/callbacks.py def run ( self , hook , * args , ** kwargs ): \"\"\" Loop through the registered actions and fire all callbacks Args: hook The name of the hook to check, defaults to all args Arguments to receive from GDRF kwargs Keyword Arguments to receive from gDRF \"\"\" assert ( hook in self . _callbacks ), f \"hook ' { hook } ' not found in callbacks { self . _callbacks } \" for logger in self . _callbacks [ hook ]: logger [ \"callback\" ]( * args , ** kwargs ) general \u00b6 General utils Adapted from YOLOv5, https://github.com/ultralytics/yolov5/ loggers \u00b6 Logging utils Adapted from YOLOv5, https://github.com/ultralytics/yolov5/ wandblogger \u00b6 Utilities and tools for tracking runs with Weights & Biases. Adapted from YOLOv5, https://github.com/ultralytics/yolov5/ WandbLogger \u00b6 Log training runs, datasets, models, and predictions to Weights & Biases. This logger sends information to W&B at wandb.ai. By default, this information includes hyperparameters, system configuration and metrics, model metrics, and basic data metrics and analyses. By providing additional command line arguments to train.py, datasets, models and predictions can also be logged. For more on how this logger is used, see the Weights & Biases documentation: https://docs.wandb.com/guides/integrations/yolov5 __init__ ( self , opt , opt_transforms = None , run_id = None , job_type = 'Training' ) special \u00b6 Initialize WandbLogger instance Upload dataset if opt.upload_dataset is True Setup trainig processes if job_type is 'Training' opt (namespace) -- Commandline arguments for this run opt_transforms (dict) -- Dictionary of transforms to apply when parsing config run_id (str) -- Run ID of W&B run to be resumed job_type (str) -- To set the job_type for this run Source code in gdrf/utils/wandblogger.py def __init__ ( self , opt , opt_transforms = None , run_id = None , job_type = \"Training\" ): \"\"\" - Initialize WandbLogger instance - Upload dataset if opt.upload_dataset is True - Setup trainig processes if job_type is 'Training' arguments: opt (namespace) -- Commandline arguments for this run opt_transforms (dict) -- Dictionary of transforms to apply when parsing config run_id (str) -- Run ID of W&B run to be resumed job_type (str) -- To set the job_type for this run \"\"\" # Pre-training routine -- self . job_type = job_type self . wandb , self . wandb_run = wandb , None if not wandb else wandb . run self . train_artifact = None self . train_artifact_path = None self . result_artifact = None self . max_imgs_to_log = 16 self . wandb_artifact_data_dict = None self . data_dict = None self . opt_transforms = opt_transforms # It's more elegant to stick to 1 wandb.init call, but useful config data is overwritten in the WandbLogger's wandb.init call if isinstance ( opt [ \"resume\" ], str ): # checks resume from artifact if opt [ \"resume\" ] . startswith ( WANDB_ARTIFACT_PREFIX ): entity , project , run_id , model_artifact_name = get_run_info ( opt [ \"resume\" ] ) model_artifact_name = WANDB_ARTIFACT_PREFIX + model_artifact_name assert wandb , \"install wandb to resume wandb runs\" # Resume wandb-artifact:// runs here| workaround for not overwriting wandb.config self . wandb_run = wandb . init ( id = run_id , project = project , entity = entity , resume = \"allow\" , allow_val_change = True , ) opt . resume = model_artifact_name elif self . wandb : self . wandb_run = ( wandb . init ( config = opt , resume = \"allow\" , project = Path ( opt [ \"project\" ]) . stem , entity = opt [ \"entity\" ], name = opt [ \"name\" ], job_type = job_type , id = run_id , allow_val_change = True , ) if not wandb . run else wandb . run ) if self . wandb_run : if self . job_type == \"Training\" : if opt [ \"upload_dataset\" ]: if not opt [ \"resume\" ]: self . wandb_artifact_data_dict = self . check_and_upload_dataset ( opt ) if opt [ \"resume\" ]: # resume from artifact if isinstance ( opt [ \"resume\" ], str ) and opt [ \"resume\" ] . startswith ( WANDB_ARTIFACT_PREFIX ): self . data_dict = dict ( self . wandb_run . config . data_dict ) else : # local resume self . data_dict = check_wandb_dataset ( opt [ \"data\" ]) else : self . data_dict = check_wandb_dataset ( opt [ \"data\" ]) self . wandb_artifact_data_dict = ( self . wandb_artifact_data_dict or self . data_dict ) # write data_dict to config. useful for resuming from artifacts. Do this only when not resuming. self . wandb_run . config . update ( { \"data_dict\" : self . wandb_artifact_data_dict }, allow_val_change = True , ) self . setup_training ( opt ) if self . job_type == \"Dataset Creation\" : self . data_dict = self . check_and_upload_dataset ( opt ) check_and_upload_dataset ( self , opt ) \u00b6 Check if the dataset format is compatible and upload it as W&B artifact opt (namespace)-- Commandline arguments for current run Updated dataset info dictionary where local dataset paths are replaced by WAND_ARFACT_PREFIX links. Source code in gdrf/utils/wandblogger.py def check_and_upload_dataset ( self , opt ): \"\"\" Check if the dataset format is compatible and upload it as W&B artifact arguments: opt (namespace)-- Commandline arguments for current run returns: Updated dataset info dictionary where local dataset paths are replaced by WAND_ARFACT_PREFIX links. \"\"\" assert wandb , \"Install wandb to upload dataset\" config_path = self . log_dataset_artifact ( opt [ \"data\" ], Path ( opt [ \"project\" ]) . stem ) print ( \"Created dataset config file \" , config_path ) with open ( config_path , errors = \"ignore\" ) as f : wandb_data_dict = yaml . safe_load ( f ) return wandb_data_dict create_dataset_table ( self , dataset , name = 'dataset' ) \u00b6 Create and return W&B artifact containing W&B Table of the dataset. dataset (pandas.DataFrame) -- Dataframe with columns representing class_to_id (dict(int, str)) -- hash map that maps class ids to labels name (str) -- name of the artifact dataset artifact to be logged or used Source code in gdrf/utils/wandblogger.py def create_dataset_table ( self , dataset , name = \"dataset\" ): \"\"\" Create and return W&B artifact containing W&B Table of the dataset. arguments: dataset (pandas.DataFrame) -- Dataframe with columns representing class_to_id (dict(int, str)) -- hash map that maps class ids to labels name (str) -- name of the artifact returns: dataset artifact to be logged or used \"\"\" artifact = wandb . Artifact ( name = name , type = \"dataset\" ) table = wandb . Table ( dataframe = dataset ) artifact . add ( table , name ) return artifact download_dataset_artifact ( self , path , alias ) \u00b6 download the model checkpoint artifact if the path starts with WANDB_ARTIFACT_PREFIX path -- path of the dataset to be used for training alias (str)-- alias of the artifact to be download/used for training (str, wandb.Artifact) -- path of the downladed dataset and it's corresponding artifact object if dataset is found otherwise returns (None, None) Source code in gdrf/utils/wandblogger.py def download_dataset_artifact ( self , path , alias ): \"\"\" download the model checkpoint artifact if the path starts with WANDB_ARTIFACT_PREFIX arguments: path -- path of the dataset to be used for training alias (str)-- alias of the artifact to be download/used for training returns: (str, wandb.Artifact) -- path of the downladed dataset and it's corresponding artifact object if dataset is found otherwise returns (None, None) \"\"\" if isinstance ( path , str ) and path . startswith ( WANDB_ARTIFACT_PREFIX ): artifact_path = Path ( remove_prefix ( path , WANDB_ARTIFACT_PREFIX ) + \":\" + alias ) dataset_artifact = wandb . use_artifact ( artifact_path . as_posix () . replace ( \" \\\\ \" , \"/\" ) ) assert ( dataset_artifact is not None ), \"'Error: W&B dataset artifact doesn't exist'\" datadir = dataset_artifact . download () return datadir , dataset_artifact return None , None download_model_artifact ( self , opt ) \u00b6 download the model checkpoint artifact if the resume path starts with WANDB_ARTIFACT_PREFIX opt (namespace) -- Commandline arguments for this run Source code in gdrf/utils/wandblogger.py def download_model_artifact ( self , opt ): \"\"\" download the model checkpoint artifact if the resume path starts with WANDB_ARTIFACT_PREFIX arguments: opt (namespace) -- Commandline arguments for this run \"\"\" if opt [ \"resume\" ] . startswith ( WANDB_ARTIFACT_PREFIX ): model_artifact = wandb . use_artifact ( remove_prefix ( opt [ \"resume\" ], WANDB_ARTIFACT_PREFIX ) + \":latest\" ) assert model_artifact is not None , \"Error: W&B model artifact doesn't exist\" modeldir = model_artifact . download () # epochs_trained = model_artifact.metadata.get(\"epochs_trained\") total_epochs = model_artifact . metadata . get ( \"total_epochs\" ) is_finished = total_epochs is None assert ( not is_finished ), \"training is finished, can only resume incomplete runs.\" return modeldir , model_artifact return None , None end_epoch ( self , best_result = False ) \u00b6 commit the log_dict, model artifacts and Tables to W&B and flush the log_dict. best_result (boolean): Boolean representing if the result of this evaluation is best or not Source code in gdrf/utils/wandblogger.py def end_epoch ( self , best_result = False ): \"\"\" commit the log_dict, model artifacts and Tables to W&B and flush the log_dict. arguments: best_result (boolean): Boolean representing if the result of this evaluation is best or not \"\"\" if self . wandb_run : with all_logging_disabled (): wandb . log ( self . log_dict ) self . log_dict = {} if self . result_artifact : wandb . log_artifact ( self . result_artifact , aliases = [ \"latest\" , \"last\" , \"epoch \" + str ( self . current_epoch ), ( \"best\" if best_result else \"\" ), ], ) self . result_artifact = wandb . Artifact ( \"run_\" + wandb . run . id + \"_progress\" , \"evaluation\" ) finish_run ( self , ** kwargs ) \u00b6 Log metrics if any and finish the current W&B run Source code in gdrf/utils/wandblogger.py def finish_run ( self , ** kwargs ): \"\"\" Log metrics if any and finish the current W&B run \"\"\" if all ( kwargs . get ( x , None ) is not None for x in [ \"artifact_or_path\" , \"type\" , \"name\" , \"aliases\" ] ): wandb . log_artifact ( artifact_or_path = kwargs . get ( \"artifact_or_path\" ), type = kwargs . get ( \"type\" ), name = kwargs . get ( \"name\" ), aliases = kwargs . get ( \"aliases\" ), ) if self . wandb_run : if self . log_dict : with all_logging_disabled (): wandb . log ( self . log_dict ) wandb . run . finish () log ( self , log_dict ) \u00b6 save the metrics to the logging dictionary log_dict (Dict) -- metrics/media to be logged in current step Source code in gdrf/utils/wandblogger.py def log ( self , log_dict ): \"\"\" save the metrics to the logging dictionary arguments: log_dict (Dict) -- metrics/media to be logged in current step \"\"\" if self . wandb_run : for key , value in log_dict . items (): self . log_dict [ key ] = value log_dataset_artifact ( self , data_file , project , overwrite_config = False ) \u00b6 Log the dataset as W&B artifact and return the new data file with W&B links data_file (str) -- the .yaml file with information about the dataset like - path, classes etc. single_class (boolean) -- train multi-class data as single-class project (str) -- project name. Used to construct the artifact path overwrite_config (boolean) -- overwrites the data.yaml file if set to true otherwise creates a new file with _wandb postfix. Eg -> data_wandb.yaml the new .yaml file with artifact links. it can be used to start training directly from artifacts Source code in gdrf/utils/wandblogger.py def log_dataset_artifact ( self , data_file , project , overwrite_config = False ): \"\"\" Log the dataset as W&B artifact and return the new data file with W&B links arguments: data_file (str) -- the .yaml file with information about the dataset like - path, classes etc. single_class (boolean) -- train multi-class data as single-class project (str) -- project name. Used to construct the artifact path overwrite_config (boolean) -- overwrites the data.yaml file if set to true otherwise creates a new file with _wandb postfix. Eg -> data_wandb.yaml returns: the new .yaml file with artifact links. it can be used to start training directly from artifacts \"\"\" self . data_dict = check_dataset ( data_file ) # parse and check data = dict ( self . data_dict ) self . train_artifact = ( self . create_dataset_table ( pd . read_csv ( data [ \"train\" ], index_col = 0 ), name = \"train\" ) if data . get ( \"train\" ) else None ) if data . get ( \"train\" ): data [ \"train\" ] = WANDB_ARTIFACT_PREFIX + str ( Path ( project ) / \"train\" ) if data . get ( \"val\" ): data [ \"val\" ] = WANDB_ARTIFACT_PREFIX + str ( Path ( project ) / \"val\" ) path = Path ( data_file ) . stem path = ( path if overwrite_config else path + \"_wandb\" ) + \".csv\" # updated data.yaml path data . pop ( \"download\" , None ) data . pop ( \"path\" , None ) if self . job_type == \"Training\" : # builds correct artifact pipeline graph self . wandb_run . use_artifact ( self . train_artifact ) else : self . wandb_run . log_artifact ( self . train_artifact ) return path log_model ( self , path , opt , epoch , fitness_score , best_model = False ) \u00b6 Log the model checkpoint as W&B artifact path (Path) -- Path of directory containing the checkpoints opt (namespace) -- Command line arguments for this run epoch (int) -- Current epoch number fitness_score (float) -- fitness score for current epoch best_model (boolean) -- Boolean representing if the current checkpoint is the best yet. Source code in gdrf/utils/wandblogger.py def log_model ( self , path , opt , epoch , fitness_score , best_model = False ): \"\"\" Log the model checkpoint as W&B artifact arguments: path (Path) -- Path of directory containing the checkpoints opt (namespace) -- Command line arguments for this run epoch (int) -- Current epoch number fitness_score (float) -- fitness score for current epoch best_model (boolean) -- Boolean representing if the current checkpoint is the best yet. \"\"\" model_artifact = wandb . Artifact ( \"run_\" + wandb . run . id + \"_model\" , type = \"model\" , metadata = { \"original_url\" : str ( path ), \"epochs_trained\" : epoch + 1 , \"save period\" : opt [ \"save_period\" ], \"project\" : opt [ \"project\" ], \"total_epochs\" : opt [ \"epochs\" ], \"fitness_score\" : fitness_score , }, ) model_artifact . add_file ( str ( path / \"last.pt\" ), name = \"last.pt\" ) wandb . log_artifact ( model_artifact , aliases = [ \"latest\" , \"last\" , \"epoch \" + str ( self . current_epoch ), \"best\" if best_model else \"\" , ], ) print ( \"Saving model artifact on epoch \" , epoch + 1 ) setup_training ( self , opt ) \u00b6 Setup the necessary processes for training YOLO models: - Attempt to download model checkpoint and dataset artifacts if opt.resume stats with WANDB_ARTIFACT_PREFIX - Update data_dict, to contain info of previous run if resumed and the paths of dataset artifact if downloaded - Setup log_dict, initialize bbox_interval opt (namespace) -- commandline arguments for this run Source code in gdrf/utils/wandblogger.py def setup_training ( self , opt ): \"\"\" Setup the necessary processes for training YOLO models: - Attempt to download model checkpoint and dataset artifacts if opt.resume stats with WANDB_ARTIFACT_PREFIX - Update data_dict, to contain info of previous run if resumed and the paths of dataset artifact if downloaded - Setup log_dict, initialize bbox_interval arguments: opt (namespace) -- commandline arguments for this run \"\"\" self . log_dict , self . current_epoch = {}, 0 if isinstance ( opt [ \"resume\" ], str ): modeldir , _ = self . download_model_artifact ( opt ) if modeldir : self . weights = Path ( modeldir ) / \"last.pt\" config = self . wandb_run . config for k , v in config . items (): if k in opt : if k in self . opt_transforms : v = self . opt_transforms [ k ]( v ) opt [ k ] = v data_dict = self . data_dict self . train_artifact_path , self . train_artifact = self . download_dataset_artifact ( data_dict . get ( \"train\" ), opt [ \"artifact_alias\" ] ) if self . train_artifact_path is not None : data_dict [ \"train\" ] = str ( Path ( self . train_artifact_path )) train_from_artifact = self . train_artifact_path is not None # Update the the data_dict to point to local artifacts dir if train_from_artifact : self . data_dict = data_dict all_logging_disabled ( highest_level = 50 ) \u00b6 source - https://gist.github.com/simon-weber/7853144 A context manager that will prevent any logging messages triggered during the body from being processed. :param highest_level: the maximum logging level in use. This would only need to be changed if a custom level greater than CRITICAL is defined. Source code in gdrf/utils/wandblogger.py @contextmanager def all_logging_disabled ( highest_level = logging . CRITICAL ): \"\"\"source - https://gist.github.com/simon-weber/7853144 A context manager that will prevent any logging messages triggered during the body from being processed. :param highest_level: the maximum logging level in use. This would only need to be changed if a custom level greater than CRITICAL is defined. \"\"\" previous_level = logging . root . manager . disable logging . disable ( highest_level ) try : yield finally : logging . disable ( previous_level ) visualize special \u00b6 matrixplot_cli ( data , log = False , epsilon = 1e-10 ) \u00b6 Creates a matrix plot from the probabilities in the CSV file data . The first row of the CSV file should be a header. The first column of the CSV file should be an index. All other columns are considered data. Optionally, the probabilities may be plotted on a log-scale colorbar. :param str data: A CSV file with probabilities. Header row is required, index column is required. :param bool log: An optional CSV file with the index. Header row is required. :param float epsilon: An optional \"jitter\" parameter, for plotting small probabilities on a log-scale Source code in gdrf/visualize/__init__.py def matrixplot_cli ( data : str , log : bool = False , epsilon : float = 1e-10 ): \"\"\" Creates a matrix plot from the probabilities in the CSV file `data`. The first row of the CSV file should be a header. The first column of the CSV file should be an index. All other columns are considered data. Optionally, the probabilities may be plotted on a log-scale colorbar. :param str data: A CSV file with probabilities. Header row is required, index column is required. :param bool log: An optional CSV file with the index. Header row is required. :param float epsilon: An optional \"jitter\" parameter, for plotting small probabilities on a log-scale \"\"\" display ( matrix_plot ( data , log = log , epsilon = epsilon )) maxplot_2d_cli ( data , index = None ) \u00b6 Creates a 2-D maximum plot from the probabilities in the CSV file data . Optionally, a two-column CSV file index may be provided. The first row of CSV files should be a header. All columns are considered data, except if no index CSV is provided the first two columns of data are the index. :param str data: A CSV file with probabilities. Header row is required, index columns are optional :param str index: An optional CSV file with the index. Header row is required. Source code in gdrf/visualize/__init__.py def maxplot_2d_cli ( data : str , index : str = None ): \"\"\" Creates a 2-D maximum plot from the probabilities in the CSV file `data`. Optionally, a two-column CSV file `index` may be provided. The first row of CSV files should be a header. All columns are considered data, except if no `index` CSV is provided the first two columns of `data` are the index. :param str data: A CSV file with probabilities. Header row is required, index columns are optional :param str index: An optional CSV file with the index. Header row is required. \"\"\" display ( maxplot_2d ( data , index )) stackplot_1d_cli ( data , index = None ) \u00b6 Creates a 1-D stackplot from the probabilities in the CSV file data . Optionally, a single-column CSV file index may be provided. The first row of CSV files should be a header. All columns are considered data, except if no index CSV is provided the first column of data is the index. :param str data: A CSV file with probabilities. Header row is required, index column is optional :param str index: An optional CSV file with the index. Header row is required. Source code in gdrf/visualize/__init__.py def stackplot_1d_cli ( data : str , index : str = None ): \"\"\" Creates a 1-D stackplot from the probabilities in the CSV file `data`. Optionally, a single-column CSV file `index` may be provided. The first row of CSV files should be a header. All columns are considered data, except if no `index` CSV is provided the first column of `data` is the index. :param str data: A CSV file with probabilities. Header row is required, index column is optional :param str index: An optional CSV file with the index. Header row is required. \"\"\" display ( stackplot_1d ( data , index ))","title":"modules"},{"location":"api/#gdrf.cli","text":"Console script for gdrf.","title":"cli"},{"location":"api/#gdrf.infer","text":"","title":"infer"},{"location":"api/#gdrf.infer.divergence","text":"","title":"divergence"},{"location":"api/#gdrf.infer.divergence.NonlinearExpectationDivergence","text":"","title":"NonlinearExpectationDivergence"},{"location":"api/#gdrf.infer.divergence.NonlinearExpectationDivergence.loss","text":":returns: returns an estimate of the ELBO :rtype: float Evaluates the ELBO with an estimator that uses num_particles many samples/particles. Source code in gdrf/infer/divergence.py def loss ( self , model , guide , * args , ** kwargs ): \"\"\" :returns: returns an estimate of the ELBO :rtype: float Evaluates the ELBO with an estimator that uses num_particles many samples/particles. \"\"\" loss_particles = [] is_vectorized = self . vectorize_particles and self . num_particles > 1 # grab a vectorized trace from the generator for model_trace , guide_trace in self . _get_traces ( model , guide , args , kwargs ): loss_particle = self . loss_fn ( model_trace , guide_trace , args , kwargs ) loss_particles . append ( loss_particle ) if is_vectorized : loss_particles = loss_particles [ 0 ] else : loss_particles = torch . stack ( loss_particles ) loss = - loss_particles . sum () . item () / self . num_particles warn_if_nan ( loss , \"loss\" ) return loss","title":"loss()"},{"location":"api/#gdrf.infer.divergence.NonlinearExpectationDivergence.loss_and_grads","text":":returns: returns an estimate of the ELBO :rtype: float Computes the ELBO as well as the surrogate ELBO that is used to form the gradient estimator. Performs backward on the latter. Num_particle many samples are used to form the estimators. Source code in gdrf/infer/divergence.py def loss_and_grads ( self , model , guide , * args , ** kwargs ): \"\"\" :returns: returns an estimate of the ELBO :rtype: float Computes the ELBO as well as the surrogate ELBO that is used to form the gradient estimator. Performs backward on the latter. Num_particle many samples are used to form the estimators. \"\"\" loss_particles = [] surrogate_loss_particles = [] is_vectorized = self . vectorize_particles and self . num_particles > 1 tensor_holder = None # grab a vectorized trace from the generator for model_trace , guide_trace in self . _get_traces ( model , guide , args , kwargs ): surrogate_loss_particle = self . loss_fn ( model_trace , guide_trace , args , kwargs ) loss_particle = surrogate_loss_particle . detach () . item () if is_identically_zero ( loss_particle ): if tensor_holder is not None : loss_particle = torch . zeros_like ( tensor_holder ) surrogate_loss_particle = torch . zeros_like ( tensor_holder ) else : # loss_particle is not None if tensor_holder is None : tensor_holder = torch . zeros_like ( loss_particle ) # change types of previous `loss_particle`s for i in range ( len ( loss_particles )): loss_particles [ i ] = torch . zeros_like ( tensor_holder ) surrogate_loss_particles [ i ] = torch . zeros_like ( tensor_holder ) loss_particles . append ( loss_particle ) surrogate_loss_particles . append ( surrogate_loss_particle ) if tensor_holder is None : return 0.0 if is_vectorized : loss_particles = loss_particles [ 0 ] surrogate_loss_particles = surrogate_loss_particles [ 0 ] else : loss_particles = torch . stack ( loss_particles ) surrogate_loss_particles = torch . stack ( surrogate_loss_particles ) divergence = self . nonlinearity ( loss_particles . sum ( dim = 0 , keepdim = True ) / self . num_particles ) # collect parameters to train from model and guide trainable_params = any ( site [ \"type\" ] == \"param\" for trace in ( model_trace , guide_trace ) for site in trace . nodes . values () ) if trainable_params and getattr ( surrogate_loss_particles , \"requires_grad\" , False ): surrogate_divergence = - self . nonlinearity ( surrogate_loss_particles . sum () / self . num_particles ) surrogate_divergence . backward () loss = - divergence warn_if_nan ( loss , \"loss\" ) return loss","title":"loss_and_grads()"},{"location":"api/#gdrf.infer.loss","text":"","title":"loss"},{"location":"api/#gdrf.infer.loss.Loss","text":"","title":"Loss"},{"location":"api/#gdrf.infer.loss.Loss.loss","text":":returns: returns an estimate of the ELBO :rtype: float Evaluates the ELBO with an estimator that uses num_particles many samples/particles. Source code in gdrf/infer/loss.py @torch . no_grad () def loss ( self , model , guide , * args , ** kwargs ): \"\"\" :returns: returns an estimate of the ELBO :rtype: float Evaluates the ELBO with an estimator that uses num_particles many samples/particles. \"\"\" loss_particles = [] is_vectorized = self . vectorize_particles and self . num_particles > 1 # grab a vectorized trace from the generator for model_trace , guide_trace in self . _get_traces ( model , guide , args , kwargs ): loss_particle = self . loss_fn ( model_trace , guide_trace , args , kwargs ) loss_particles . append ( loss_particle ) if is_vectorized : loss_particles = loss_particles [ 0 ] else : loss_particles = torch . stack ( loss_particles ) loss = - loss_particles . sum () . item () / self . num_particles warn_if_nan ( loss , \"loss\" ) return loss","title":"loss()"},{"location":"api/#gdrf.infer.loss.Loss.loss_and_grads","text":":returns: returns an estimate of the ELBO :rtype: float Computes the ELBO as well as the surrogate ELBO that is used to form the gradient estimator. Performs backward on the latter. Num_particle many samples are used to form the estimators. Source code in gdrf/infer/loss.py def loss_and_grads ( self , model , guide , * args , ** kwargs ): \"\"\" :returns: returns an estimate of the ELBO :rtype: float Computes the ELBO as well as the surrogate ELBO that is used to form the gradient estimator. Performs backward on the latter. Num_particle many samples are used to form the estimators. \"\"\" loss_particles = [] surrogate_loss_particles = [] is_vectorized = self . vectorize_particles and self . num_particles > 1 tensor_holder = None # grab a vectorized trace from the generator for model_trace , guide_trace in self . _get_traces ( model , guide , args , kwargs ): surrogate_loss_particle = self . loss_fn ( model_trace , guide_trace , args , kwargs ) loss_particle = surrogate_loss_particle . detach () . item () if is_identically_zero ( loss_particle ): if tensor_holder is not None : loss_particle = torch . zeros_like ( tensor_holder ) surrogate_loss_particle = torch . zeros_like ( tensor_holder ) else : # loss_particle is not None if tensor_holder is None : tensor_holder = torch . zeros_like ( loss_particle ) # change types of previous `loss_particle`s for i in range ( len ( loss_particles )): loss_particles [ i ] = torch . zeros_like ( tensor_holder ) surrogate_loss_particles [ i ] = torch . zeros_like ( tensor_holder ) loss_particles . append ( loss_particle ) surrogate_loss_particles . append ( surrogate_loss_particle ) if tensor_holder is None : return 0.0 if is_vectorized : loss_particles = loss_particles [ 0 ] surrogate_loss_particles = surrogate_loss_particles [ 0 ] else : loss_particles = torch . stack ( loss_particles ) surrogate_loss_particles = torch . stack ( surrogate_loss_particles ) loss_val = loss_particles . sum ( dim = 0 , keepdim = True ) / self . num_particles # collect parameters to train from model and guide trainable_params = any ( site [ \"type\" ] == \"param\" for trace in ( model_trace , guide_trace ) for site in trace . nodes . values () ) if trainable_params and getattr ( surrogate_loss_particles , \"requires_grad\" , False ): surrogate_loss_val = - surrogate_loss_particles . sum () / self . num_particles surrogate_loss_val . backward () loss = - loss_val warn_if_nan ( loss , \"loss\" ) return loss","title":"loss_and_grads()"},{"location":"api/#gdrf.models","text":"","title":"models"},{"location":"api/#gdrf.models.gdrf","text":"","title":"gdrf"},{"location":"api/#gdrf.models.gdrf.GDRF","text":"","title":"GDRF"},{"location":"api/#gdrf.models.gdrf.GDRF.forward","text":"Computes the mean and covariance matrix (or variance) of Gaussian Process posterior on a test input data :math: X_{new} : .. math:: p(f^* \\mid X_{new}, X, y, k, f_{loc}, f_{scale_tril}) = \\mathcal{N}(loc, cov). .. note:: Variational parameters f_loc , f_scale_tril , together with kernel's parameters have been learned from a training procedure (MCMC or SVI). :param torch.Tensor Xnew: A input data for testing. Note that Xnew.shape[1:] must be the same as self.X.shape[1:] . :param bool full_cov: A flag to decide if we want to predict full covariance matrix or just variance. :returns: loc and covariance matrix (or variance) of :math: p(f^*(X_{new})) :rtype: tuple(torch.Tensor, torch.Tensor) Source code in gdrf/models/gdrf.py @nn . pyro_method @scale_decorator ( \"Xnew\" ) def forward ( self , Xnew , full_cov = False ): r \"\"\" Computes the mean and covariance matrix (or variance) of Gaussian Process posterior on a test input data :math:`X_{new}`: .. math:: p(f^* \\mid X_{new}, X, y, k, f_{loc}, f_{scale\\_tril}) = \\mathcal{N}(loc, cov). .. note:: Variational parameters ``f_loc``, ``f_scale_tril``, together with kernel's parameters have been learned from a training procedure (MCMC or SVI). :param torch.Tensor Xnew: A input data for testing. Note that ``Xnew.shape[1:]`` must be the same as ``self.X.shape[1:]``. :param bool full_cov: A flag to decide if we want to predict full covariance matrix or just variance. :returns: loc and covariance matrix (or variance) of :math:`p(f^*(X_{new}))` :rtype: tuple(torch.Tensor, torch.Tensor) \"\"\" self . _check_Xnew_shape ( Xnew ) self . set_mode ( \"guide\" ) loc , cov = gp . util . conditional ( Xnew , self . scale ( self . xs ), self . _kernel , self . f_loc , self . f_scale_tril , full_cov = full_cov , whiten = self . _whiten , jitter = self . _jitter , ) return loc + self . _mean_function ( Xnew ), cov","title":"forward()"},{"location":"api/#gdrf.models.sparse_gdrf","text":"","title":"sparse_gdrf"},{"location":"api/#gdrf.models.sparse_gdrf.SparseGDRF","text":"","title":"SparseGDRF"},{"location":"api/#gdrf.models.sparse_gdrf.SparseGDRF.forward","text":"Computes the mean and covariance matrix (or variance) of Gaussian Process posterior on a test input data :math: X_{new} : .. math:: p(f^* \\mid X_{new}, X, y, k, f_{loc}, f_{scale_tril}) = \\mathcal{N}(loc, cov). .. note:: Variational parameters f_loc , f_scale_tril , together with kernel's parameters have been learned from a training procedure (MCMC or SVI). :param torch.Tensor Xnew: A input data for testing. Note that Xnew.shape[1:] must be the same as self.X.shape[1:] . :param bool full_cov: A flag to decide if we want to predict full covariance matrix or just variance. :returns: loc and covariance matrix (or variance) of :math: p(f^*(X_{new})) :rtype: tuple(torch.Tensor, torch.Tensor) Source code in gdrf/models/sparse_gdrf.py @scale_decorator ( \"Xnew\" ) def forward ( self , Xnew , full_cov = False ): r \"\"\" Computes the mean and covariance matrix (or variance) of Gaussian Process posterior on a test input data :math:`X_{new}`: .. math:: p(f^* \\mid X_{new}, X, y, k, f_{loc}, f_{scale\\_tril}) = \\mathcal{N}(loc, cov). .. note:: Variational parameters ``f_loc``, ``f_scale_tril``, together with kernel's parameters have been learned from a training procedure (MCMC or SVI). :param torch.Tensor Xnew: A input data for testing. Note that ``Xnew.shape[1:]`` must be the same as ``self.X.shape[1:]``. :param bool full_cov: A flag to decide if we want to predict full covariance matrix or just variance. :returns: loc and covariance matrix (or variance) of :math:`p(f^*(X_{new}))` :rtype: tuple(torch.Tensor, torch.Tensor) \"\"\" self . set_mode ( \"guide\" ) posterior_kernel = self . _kernel posterior_u_loc = self . u_loc posterior_u_scale_tril = self . u_scale_tril Luu = jittercholesky ( posterior_kernel ( torch . Tensor ( self . _inducing_points )) . contiguous (), self . M , self . jitter , self . maxjitter , ) f_loc , f_cov = gp . util . conditional ( Xnew , torch . Tensor ( self . _inducing_points ), posterior_kernel , torch . Tensor ( posterior_u_loc ), torch . Tensor ( posterior_u_scale_tril ), Luu , full_cov = full_cov , whiten = self . _whiten , jitter = self . _jitter , ) return f_loc + self . _mean_function ( Xnew ), f_cov","title":"forward()"},{"location":"api/#gdrf.models.topic_model","text":"","title":"topic_model"},{"location":"api/#gdrf.models.topic_model.CategoricalModel","text":"","title":"CategoricalModel"},{"location":"api/#gdrf.models.topic_model.CategoricalModel.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in gdrf/models/topic_model.py def forward ( self , input : torch . Tensor ) -> torch . Tensor : self . set_mode ( \"guide\" ) return self . word_probs ( indices = input )","title":"forward()"},{"location":"api/#gdrf.models.topic_model.SpatioTemporalTopicModel","text":"","title":"SpatioTemporalTopicModel"},{"location":"api/#gdrf.models.topic_model.SpatioTemporalTopicModel.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in gdrf/models/topic_model.py @abstractmethod def forward ( self , input : torch . Tensor ) -> torch . Tensor : pass","title":"forward()"},{"location":"api/#gdrf.models.utils","text":"","title":"utils"},{"location":"api/#gdrf.models.utils.generate_data_2d_circles","text":"This function generates an artifical 2D dataset with K total topics and V total words. Over a background topic with a uniform distribution of words, one-topic circles are scattered. Each non-background topic has a word distribution a la the Girdhar thesis word distributions. At each pixel, a random number of observations, uniformly distributed between V and 10V , is generated. Source code in gdrf/models/utils.py def generate_data_2d_circles ( K = 5 , V = 100 , N = 8 , W = 150 , H = 32 , R = 8 , eta = 0.1 , seed = 777 , device = \"cpu\" , permute = False , constant_background = True , ): \"\"\" This function generates an artifical 2D dataset with `K` total topics and `V` total words. Over a background topic with a uniform distribution of words, one-topic circles are scattered. Each non-background topic has a word distribution a la the Girdhar thesis word distributions. At each pixel, a random number of observations, uniformly distributed between `V` and `10V`, is generated. \"\"\" K_obj = K - 1 gen = torch . Generator ( device = device ) . manual_seed ( seed ) gen . manual_seed ( seed ) topics = torch . zeros (( W , H ), dtype = torch . int , device = device ) centers_x = torch . randint ( R , W - R , [ N ], device = device , generator = gen ) centers_y = torch . randint ( R , H - R , [ N ], device = device , generator = gen ) centers = torch . stack ([ centers_x , centers_y ]) . T obj_topics = torch . randint ( 0 , K_obj , [ N ], device = device , generator = gen ) xs = torch . reshape ( torch . stack ( torch . meshgrid ( torch . tensor ( list ( range ( W )), device = device ), torch . tensor ( list ( range ( H )), device = device ), ) ), ( 2 , W * H ), ) . T counts = torch . randint ( low = V , high = 10 * V , size = ( W * H ,), device = device , generator = gen ) while len ( obj_topics . unique ()) < K_obj and N >= K : obj_topics = torch . randint ( 0 , K_obj , [ N ], device = device , generator = gen ) obj_topics += 1 K_obj_probs = K_obj if constant_background else K p_v_z = torch . tensor ( [ [ 1 + eta if V * k / K_obj_probs <= v < V * ( k + 1 ) / K_obj_probs else eta for v in range ( V ) ] for k in range ( K_obj ) ], device = device , ) words = torch . zeros (( W * H , V ), dtype = torch . int , device = device ) probs = words * 0.0 if constant_background : bg_dist = torch . Tensor ([ 1 / V for _ in range ( V )]) . to ( probs . device ) else : bg_dist = torch . Tensor ( [ 1 + eta if V * ( K - 1 ) / K <= v <= V else eta for v in range ( V )] ) . to ( probs . device ) p_v_z = torch . cat ([ p_v_z , bg_dist . unsqueeze ( 0 )], dim = 0 ) p_v_z /= p_v_z . sum ( dim =- 1 , keepdim = True ) # p_v_z.shape = (K, V)] if permute : idx = torch . randperm ( p_v_z . shape [ 1 ]) p_v_z = p_v_z [:, idx ] . view ( p_v_z . size ()) probs = probs + p_v_z [ - 1 , :] for idx in range ( W * H ): w , h = xs [ idx , :] for i in range ( centers . shape [ 0 ]): diff = xs [ idx , :] - centers [ i , :] if ( diff * diff ) . sum () <= R * R : topics [ w , h ] = obj_topics [ i ] probs [ idx , :] = p_v_z [ obj_topics [ i ] - 1 , :] break words = torch . zeros (( W * H , V ), dtype = torch . int , device = device ) for idx in range ( len ( counts )): cats , obs = torch . multinomial ( input = probs [ idx , :], num_samples = counts [ idx ], replacement = True , generator = gen , ) . unique ( return_counts = True ) for i , cat in enumerate ( cats ): words [ idx , cat ] = obs [ i ] return centers , topics , p_v_z , xs , words","title":"generate_data_2d_circles()"},{"location":"api/#gdrf.models.utils.generate_data_2d_uniform","text":"This function generates an artifical 2D dataset with V total words. Word distributions are completely uniform everywhere. At each pixel, a random number of observations, uniformly distributed between V and 10V , is generated. Source code in gdrf/models/utils.py def generate_data_2d_uniform ( V = 100 , W = 150 , H = 32 , seed = 777 , device = \"cpu\" ): \"\"\" This function generates an artifical 2D dataset with `V` total words. Word distributions are completely uniform everywhere. At each pixel, a random number of observations, uniformly distributed between `V` and `10V`, is generated. \"\"\" gen = torch . Generator ( device = device ) . manual_seed ( seed ) gen . manual_seed ( seed ) xs = torch . reshape ( torch . stack ( torch . meshgrid ( torch . tensor ( list ( range ( W )), device = device ), torch . tensor ( list ( range ( H )), device = device ), ) ), ( 2 , W * H ), ) . T counts = torch . randint ( low = V , high = 10 * V , size = ( W * H ,), device = device , generator = gen ) words = torch . zeros (( W * H , V ), dtype = torch . int , device = device ) probs = words + 1.0 / V for idx in range ( len ( counts )): cats , obs = torch . multinomial ( input = probs [ idx , :], num_samples = counts [ idx ], replacement = True , generator = gen , ) . unique ( return_counts = True ) for i , cat in enumerate ( cats ): words [ idx , cat ] = obs [ i ] return xs , words","title":"generate_data_2d_uniform()"},{"location":"api/#gdrf.models.visualize","text":"","title":"visualize"},{"location":"api/#gdrf.models.visualize.png_bytes_to_numpy","text":"Convert png bytes to numpy array Source: https://gist.github.com/eric-czech/fea266e546efac0e704d99837a52b35f Credit: Eric Czech Source code in gdrf/models/visualize.py def png_bytes_to_numpy ( png ): \"\"\"Convert png bytes to numpy array Source: https://gist.github.com/eric-czech/fea266e546efac0e704d99837a52b35f Credit: Eric Czech \"\"\" return np . array ( Image . open ( BytesIO ( png )))","title":"png_bytes_to_numpy()"},{"location":"api/#gdrf.train","text":"Train a GDRF model on a custom dataset Usage $ python path/to/train.py --data mvco.csv","title":"train"},{"location":"api/#gdrf.train.train","text":"Trains a GDRF :param Union[str, dict] cfg: Config file or dict for training run with model and training hyperparameters :param str project: Project name for training run :param str name: Run name for training run :param str device: Device to store tensors on during training (e.g. 'cpu' or 'cuda:0') :param bool exist_ok: Existing project/name ok, do not increment :param str weights: Weights.pt file, if starting from pretrained weights :param str data: Data.csv file, with first row as column names and first dimensions columns as indexes :param int dimensions: Number of non-stationary index dimensions (e.g. a time series is 1D, x-y data are 2D, etc.) :param int epochs: Number of training epochs :param Union[str, bool] resume: Resume most recent training :param bool nosave: only save final checkpoint :param str entity: W&B entity :param bool upload_dataset: Upload dataset to W&B :param int save_period: Log model after this many epochs :param str artifact_alias: version of dataset artifact to be used :param int patience: EarlyStopping Patience (number of epochs without improvement before early stopping) :param bool verbose: Verbose Source code in gdrf/train.py def train ( # noqa: C901 cfg : Union [ str , dict ] = \"data/cfg.yaml\" , project : str = \"wandb/mvco\" , name : str = \"mvco_adamax_grid\" , device : str = \"cuda:0\" , exist_ok : bool = True , weights : str = \"\" , data : str = \"data/data.csv\" , dimensions : int = 1 , epochs : int = 3000 , resume : Union [ str , bool ] = False , nosave : bool = False , entity : str = None , upload_dataset : bool = False , save_period : int = - 1 , artifact_alias : str = \"latest\" , patience : int = 100 , verbose : bool = True , ): \"\"\" Trains a GDRF :param Union[str, dict] cfg: Config file or dict for training run with model and training hyperparameters :param str project: Project name for training run :param str name: Run name for training run :param str device: Device to store tensors on during training (e.g. 'cpu' or 'cuda:0') :param bool exist_ok: Existing project/name ok, do not increment :param str weights: Weights.pt file, if starting from pretrained weights :param str data: Data.csv file, with first row as column names and first ``dimensions`` columns as indexes :param int dimensions: Number of non-stationary index dimensions (e.g. a time series is 1D, x-y data are 2D, etc.) :param int epochs: Number of training epochs :param Union[str, bool] resume: Resume most recent training :param bool nosave: only save final checkpoint :param str entity: W&B entity :param bool upload_dataset: Upload dataset to W&B :param int save_period: Log model after this many epochs :param str artifact_alias: version of dataset artifact to be used :param int patience: EarlyStopping Patience (number of epochs without improvement before early stopping) :param bool verbose: Verbose \"\"\" opt = locals () callbacks = Callbacks () save_dir = Path ( str ( increment_path ( Path ( project ) / name , exist_ok = exist_ok ))) set_logging ( verbose = verbose ) print ( colorstr ( \"train: \" ) + \", \" . join ( f \" { k } = { v } \" for k , v in opt . items ())) check_git_status () check_requirements ( requirements = FILE . parent / \"requirements.txt\" , exclude = []) # Resume if resume and not check_wandb_resume ( resume ): # resume an interrupted run ckpt = ( resume if isinstance ( resume , str ) else get_latest_run () ) # specified or most recent path assert os . path . isfile ( ckpt ), \"ERROR: --resume checkpoint does not exist\" with open ( Path ( ckpt ) . parent . parent / \"opt.yaml\" ) as f : assert os . path . isfile ( Path ( ckpt ) . parent . parent / \"opt.yaml\" ) opt = yaml . safe_load ( f ) # replace with open ( Path ( ckpt ) . parent . parent / \"cfg.yaml\" ) as f : assert os . path . isfile ( Path ( ckpt ) . parent . parent / \"cfg.yaml\" ) cfg = yaml . safe_load ( f ) # replace opt [ \"cfg\" ], opt [ \"weights\" ], opt [ \"resume\" ] = cfg , ckpt , True weights = ckpt resume = True LOGGER . info ( f \"Resuming training from { ckpt } \" ) else : data = check_file ( data ) cfg = check_file ( cfg ) if isinstance ( cfg , str ): with open ( cfg ) as f : cfg = yaml . safe_load ( f ) # load hyps dict model_type = cfg [ \"model\" ][ \"type\" ] model_hyp = cfg [ \"model\" ][ \"hyperparameters\" ] kernel_type = cfg [ \"kernel\" ][ \"type\" ] kernel_hyp = cfg [ \"kernel\" ][ \"hyperparameters\" ] optimizer_type = cfg [ \"optimizer\" ][ \"type\" ] optimizer_hyp = cfg [ \"optimizer\" ][ \"hyperparameters\" ] objective_type = cfg [ \"objective\" ][ \"type\" ] objective_hyp = cfg [ \"objective\" ][ \"hyperparameters\" ] device = select_device ( device ) pretrained = weights . endswith ( \".pt\" ) # Directories w = save_dir / \"weights\" # weights dir w . mkdir ( parents = True , exist_ok = True ) # make dir last , best = w / \"last.pt\" , w / \"best.pt\" # Hyperparameters LOGGER . info ( colorstr ( \"config: \" ) + \", \" . join ( f \" { k } = { v } \" for k , v in cfg . items ())) # Save run settings with open ( save_dir / \"cfg.yaml\" , \"w\" ) as f : yaml . safe_dump ( cfg , f , sort_keys = False ) with open ( save_dir / \"opt.yaml\" , \"w\" ) as f : yaml . safe_dump ( opt , f , sort_keys = False ) data_dict = None # Loggers loggers = Loggers ( save_dir , weights , opt , cfg , LOGGER ) # loggers instance if loggers . wandb : data_dict = loggers . wandb . data_dict if resume : epochs , hyp = epochs , model_hyp # Register actions for k in methods ( loggers ): callbacks . register_action ( k , callback = getattr ( loggers , k )) # Config cuda = device != \"cpu\" init_seeds ( 1 ) data_dict = data_dict or check_dataset ( data ) # check if None train_path = data_dict [ \"train\" ] # Dataset dataset = ( pd . read_csv ( filepath_or_buffer = data , index_col = list ( range ( dimensions )), header = 0 , parse_dates = True , ) . fillna ( 0 ) . astype ( int ) ) index = dataset . index index = index . values if dimensions == 1 else np . array ( index . to_list ()) index = index - index . min ( axis =- dimensions , keepdims = True ) index = index / index . max ( axis =- dimensions , keepdims = True ) xs = torch . from_numpy ( index ) . float () . to ( device ) if dimensions == 1 : xs = xs . unsqueeze ( - 1 ) ws = torch . from_numpy ( dataset . values ) . int () . to ( device ) min_xs = xs . min ( dim = 0 ) . values . detach () . cpu () . numpy () . tolist () max_xs = xs . max ( dim = 0 ) . values . detach () . cpu () . numpy () . tolist () world = list ( zip ( min_xs , max_xs )) num_observation_categories = len ( dataset . columns ) # Kernel for k , v in kernel_hyp . items (): if isinstance ( v , float ): kernel_hyp [ k ] = torch . tensor ( v ) . to ( device ) kernel = KERNEL_DICT [ kernel_type ]( input_dim = dimensions , ** kernel_hyp ) kernel = kernel . to ( device ) # Model model = GDRF_MODEL_DICT [ model_type ]( xs = xs , ws = ws , world = world , kernel = kernel , num_observation_categories = num_observation_categories , device = device , ** model_hyp , ) # Optimizer optimizer = OPTIMIZER_DICT [ optimizer_type ]( optim_args = optimizer_hyp ) # Variational Objective objective = OBJECTIVE_DICT [ objective_type ]( vectorize_particles = True , ** objective_hyp ) start_epoch , best_fitness = 0 , float ( \"-inf\" ) if pretrained : ckpt = torch . load ( weights , map_location = device ) # load checkpoint exclude = [] # exclude keys csd = ckpt [ \"model\" ] . float () . state_dict () # checkpoint state_dict as FP32 csd = intersect_dicts ( csd , model . state_dict (), exclude = exclude ) # intersect model . load_state_dict ( csd , strict = False ) # load LOGGER . info ( f \"Transferred { len ( csd ) } / { len ( model . state_dict ()) } items from { weights } \" ) if ckpt [ \"optimizer\" ] is not None : optimizer . set_state ( ckpt [ \"optimizer\" ]) best_fitness = ckpt [ \"best_fitness\" ] # Epochs start_epoch = ckpt [ \"epoch\" ] + 1 if resume : assert ( start_epoch > 0 ), f \" { weights } training to { epochs } epochs is finished, nothing to resume.\" if epochs < start_epoch : LOGGER . info ( f \" { weights } has been trained for { ckpt [ 'epoch' ] } epochs. Fine-tuning for { epochs } more epochs.\" ) epochs += ckpt [ \"epoch\" ] # finetune additional epochs del ckpt , csd # SVI object scale = pyro . poutine . scale ( scale = 1.0 / len ( xs )) svi = pyro . infer . SVI ( model = scale ( model . model ), guide = scale ( model . guide ), optim = optimizer , loss = objective , ) LOGGER . info ( f \" { colorstr ( 'model:' ) } { type ( model ) . __name__ } \" ) LOGGER . info ( f \" { colorstr ( 'optimizer:' ) } { type ( optimizer ) . __name__ } \" ) LOGGER . info ( f \" { colorstr ( 'objective:' ) } { type ( objective ) . __name__ } \" ) # Resume callbacks . run ( \"on_pretrain_routine_end\" ) # Model parameters # Start training t0 = time . time () stopper = EarlyStopping ( best_fitness = best_fitness , patience = patience ) LOGGER . info ( f \"Logging results to { colorstr ( 'bold' , save_dir ) } \\n \" f \"Starting training for { epochs } epochs...\" ) with logging_redirect_tqdm (): pbar = trange ( start_epoch , epochs , initial = start_epoch , total = epochs ) for ( epoch ) in ( pbar ): # epoch ------------------------------------------------------------------ model . train () loss = svi . step ( xs , ws , subsample = False ) model . eval () perplexity = model . perplexity ( xs , ws ) . item () pbar . set_description ( f \"Epoch { epoch + 1 } \" ) pbar . set_postfix ( loss = loss , perplexity = perplexity ) callbacks . run ( \"on_train_epoch_end\" , epoch = epoch ) log_vals = [ loss , perplexity , model . kernel_lengthscale , model . kernel_variance , ] fi = - perplexity if fi > best_fitness : best_fitness = fi callbacks . run ( \"on_fit_epoch_end\" , log_vals , epoch , best_fitness , fi ) final_epoch = ( epoch + 1 == epochs ) or stopper . possible_stop if ( not nosave ) or final_epoch : ckpt = { \"epoch\" : epoch , \"best_fitness\" : best_fitness , \"model\" : deepcopy ( model ) . half (), \"optimizer\" : optimizer . get_state (), \"wandb_id\" : loggers . wandb . wandb_run . id if loggers . wandb else None , } torch . save ( ckpt , last ) if best_fitness == fi : torch . save ( ckpt , best ) del ckpt callbacks . run ( \"on_model_save\" , last , epoch , final_epoch , best_fitness , fi ) if stopper ( epoch = epoch - start_epoch , fitness = fi ): break # end epoch --------------------------------------------------------------------------------------- LOGGER . info ( f \" \\n { epoch - start_epoch + 1 } epochs completed in { ( time . time () - t0 ) / 3600 : .3f } hours.\" ) for f in last , best : if f . exists (): strip_optimizer ( f ) # strip optimizers callbacks . run ( \"on_train_end\" , last , best , xs , ws , dataset . index , dataset . columns , epoch ) LOGGER . info ( f \"Results saved to { colorstr ( 'bold' , save_dir ) } \" ) torch . cuda . empty_cache () return None","title":"train()"},{"location":"api/#gdrf.utils","text":"","title":"utils"},{"location":"api/#gdrf.utils.callbacks","text":"","title":"callbacks"},{"location":"api/#gdrf.utils.callbacks.Callbacks","text":"\" Handles all registered callbacks for model hooks Adapted from YOLOv5, https://github.com/ultralytics/yolov5/","title":"Callbacks"},{"location":"api/#gdrf.utils.callbacks.Callbacks.get_registered_actions","text":"\" Returns all the registered actions by callback hook Source code in gdrf/utils/callbacks.py def get_registered_actions ( self , hook = None ): \"\"\" \" Returns all the registered actions by callback hook Args: hook The name of the hook to check, defaults to all \"\"\" if hook : return self . _callbacks [ hook ] else : return self . _callbacks","title":"get_registered_actions()"},{"location":"api/#gdrf.utils.callbacks.Callbacks.register_action","text":"Register a new action to a callback hook Source code in gdrf/utils/callbacks.py def register_action ( self , hook , name = \"\" , callback = None ): \"\"\" Register a new action to a callback hook Args: hook The callback hook name to register the action to name The name of the action for later reference callback The callback to fire \"\"\" assert ( hook in self . _callbacks ), f \"hook ' { hook } ' not found in callbacks { self . _callbacks } \" assert callable ( callback ), f \"callback ' { callback } ' is not callable\" self . _callbacks [ hook ] . append ({ \"name\" : name , \"callback\" : callback })","title":"register_action()"},{"location":"api/#gdrf.utils.callbacks.Callbacks.run","text":"Loop through the registered actions and fire all callbacks Source code in gdrf/utils/callbacks.py def run ( self , hook , * args , ** kwargs ): \"\"\" Loop through the registered actions and fire all callbacks Args: hook The name of the hook to check, defaults to all args Arguments to receive from GDRF kwargs Keyword Arguments to receive from gDRF \"\"\" assert ( hook in self . _callbacks ), f \"hook ' { hook } ' not found in callbacks { self . _callbacks } \" for logger in self . _callbacks [ hook ]: logger [ \"callback\" ]( * args , ** kwargs )","title":"run()"},{"location":"api/#gdrf.utils.general","text":"General utils Adapted from YOLOv5, https://github.com/ultralytics/yolov5/","title":"general"},{"location":"api/#gdrf.utils.loggers","text":"Logging utils Adapted from YOLOv5, https://github.com/ultralytics/yolov5/","title":"loggers"},{"location":"api/#gdrf.utils.wandblogger","text":"Utilities and tools for tracking runs with Weights & Biases. Adapted from YOLOv5, https://github.com/ultralytics/yolov5/","title":"wandblogger"},{"location":"api/#gdrf.utils.wandblogger.WandbLogger","text":"Log training runs, datasets, models, and predictions to Weights & Biases. This logger sends information to W&B at wandb.ai. By default, this information includes hyperparameters, system configuration and metrics, model metrics, and basic data metrics and analyses. By providing additional command line arguments to train.py, datasets, models and predictions can also be logged. For more on how this logger is used, see the Weights & Biases documentation: https://docs.wandb.com/guides/integrations/yolov5","title":"WandbLogger"},{"location":"api/#gdrf.utils.wandblogger.WandbLogger.__init__","text":"Initialize WandbLogger instance Upload dataset if opt.upload_dataset is True Setup trainig processes if job_type is 'Training' opt (namespace) -- Commandline arguments for this run opt_transforms (dict) -- Dictionary of transforms to apply when parsing config run_id (str) -- Run ID of W&B run to be resumed job_type (str) -- To set the job_type for this run Source code in gdrf/utils/wandblogger.py def __init__ ( self , opt , opt_transforms = None , run_id = None , job_type = \"Training\" ): \"\"\" - Initialize WandbLogger instance - Upload dataset if opt.upload_dataset is True - Setup trainig processes if job_type is 'Training' arguments: opt (namespace) -- Commandline arguments for this run opt_transforms (dict) -- Dictionary of transforms to apply when parsing config run_id (str) -- Run ID of W&B run to be resumed job_type (str) -- To set the job_type for this run \"\"\" # Pre-training routine -- self . job_type = job_type self . wandb , self . wandb_run = wandb , None if not wandb else wandb . run self . train_artifact = None self . train_artifact_path = None self . result_artifact = None self . max_imgs_to_log = 16 self . wandb_artifact_data_dict = None self . data_dict = None self . opt_transforms = opt_transforms # It's more elegant to stick to 1 wandb.init call, but useful config data is overwritten in the WandbLogger's wandb.init call if isinstance ( opt [ \"resume\" ], str ): # checks resume from artifact if opt [ \"resume\" ] . startswith ( WANDB_ARTIFACT_PREFIX ): entity , project , run_id , model_artifact_name = get_run_info ( opt [ \"resume\" ] ) model_artifact_name = WANDB_ARTIFACT_PREFIX + model_artifact_name assert wandb , \"install wandb to resume wandb runs\" # Resume wandb-artifact:// runs here| workaround for not overwriting wandb.config self . wandb_run = wandb . init ( id = run_id , project = project , entity = entity , resume = \"allow\" , allow_val_change = True , ) opt . resume = model_artifact_name elif self . wandb : self . wandb_run = ( wandb . init ( config = opt , resume = \"allow\" , project = Path ( opt [ \"project\" ]) . stem , entity = opt [ \"entity\" ], name = opt [ \"name\" ], job_type = job_type , id = run_id , allow_val_change = True , ) if not wandb . run else wandb . run ) if self . wandb_run : if self . job_type == \"Training\" : if opt [ \"upload_dataset\" ]: if not opt [ \"resume\" ]: self . wandb_artifact_data_dict = self . check_and_upload_dataset ( opt ) if opt [ \"resume\" ]: # resume from artifact if isinstance ( opt [ \"resume\" ], str ) and opt [ \"resume\" ] . startswith ( WANDB_ARTIFACT_PREFIX ): self . data_dict = dict ( self . wandb_run . config . data_dict ) else : # local resume self . data_dict = check_wandb_dataset ( opt [ \"data\" ]) else : self . data_dict = check_wandb_dataset ( opt [ \"data\" ]) self . wandb_artifact_data_dict = ( self . wandb_artifact_data_dict or self . data_dict ) # write data_dict to config. useful for resuming from artifacts. Do this only when not resuming. self . wandb_run . config . update ( { \"data_dict\" : self . wandb_artifact_data_dict }, allow_val_change = True , ) self . setup_training ( opt ) if self . job_type == \"Dataset Creation\" : self . data_dict = self . check_and_upload_dataset ( opt )","title":"__init__()"},{"location":"api/#gdrf.utils.wandblogger.WandbLogger.check_and_upload_dataset","text":"Check if the dataset format is compatible and upload it as W&B artifact opt (namespace)-- Commandline arguments for current run Updated dataset info dictionary where local dataset paths are replaced by WAND_ARFACT_PREFIX links. Source code in gdrf/utils/wandblogger.py def check_and_upload_dataset ( self , opt ): \"\"\" Check if the dataset format is compatible and upload it as W&B artifact arguments: opt (namespace)-- Commandline arguments for current run returns: Updated dataset info dictionary where local dataset paths are replaced by WAND_ARFACT_PREFIX links. \"\"\" assert wandb , \"Install wandb to upload dataset\" config_path = self . log_dataset_artifact ( opt [ \"data\" ], Path ( opt [ \"project\" ]) . stem ) print ( \"Created dataset config file \" , config_path ) with open ( config_path , errors = \"ignore\" ) as f : wandb_data_dict = yaml . safe_load ( f ) return wandb_data_dict","title":"check_and_upload_dataset()"},{"location":"api/#gdrf.utils.wandblogger.WandbLogger.create_dataset_table","text":"Create and return W&B artifact containing W&B Table of the dataset. dataset (pandas.DataFrame) -- Dataframe with columns representing class_to_id (dict(int, str)) -- hash map that maps class ids to labels name (str) -- name of the artifact dataset artifact to be logged or used Source code in gdrf/utils/wandblogger.py def create_dataset_table ( self , dataset , name = \"dataset\" ): \"\"\" Create and return W&B artifact containing W&B Table of the dataset. arguments: dataset (pandas.DataFrame) -- Dataframe with columns representing class_to_id (dict(int, str)) -- hash map that maps class ids to labels name (str) -- name of the artifact returns: dataset artifact to be logged or used \"\"\" artifact = wandb . Artifact ( name = name , type = \"dataset\" ) table = wandb . Table ( dataframe = dataset ) artifact . add ( table , name ) return artifact","title":"create_dataset_table()"},{"location":"api/#gdrf.utils.wandblogger.WandbLogger.download_dataset_artifact","text":"download the model checkpoint artifact if the path starts with WANDB_ARTIFACT_PREFIX path -- path of the dataset to be used for training alias (str)-- alias of the artifact to be download/used for training (str, wandb.Artifact) -- path of the downladed dataset and it's corresponding artifact object if dataset is found otherwise returns (None, None) Source code in gdrf/utils/wandblogger.py def download_dataset_artifact ( self , path , alias ): \"\"\" download the model checkpoint artifact if the path starts with WANDB_ARTIFACT_PREFIX arguments: path -- path of the dataset to be used for training alias (str)-- alias of the artifact to be download/used for training returns: (str, wandb.Artifact) -- path of the downladed dataset and it's corresponding artifact object if dataset is found otherwise returns (None, None) \"\"\" if isinstance ( path , str ) and path . startswith ( WANDB_ARTIFACT_PREFIX ): artifact_path = Path ( remove_prefix ( path , WANDB_ARTIFACT_PREFIX ) + \":\" + alias ) dataset_artifact = wandb . use_artifact ( artifact_path . as_posix () . replace ( \" \\\\ \" , \"/\" ) ) assert ( dataset_artifact is not None ), \"'Error: W&B dataset artifact doesn't exist'\" datadir = dataset_artifact . download () return datadir , dataset_artifact return None , None","title":"download_dataset_artifact()"},{"location":"api/#gdrf.utils.wandblogger.WandbLogger.download_model_artifact","text":"download the model checkpoint artifact if the resume path starts with WANDB_ARTIFACT_PREFIX opt (namespace) -- Commandline arguments for this run Source code in gdrf/utils/wandblogger.py def download_model_artifact ( self , opt ): \"\"\" download the model checkpoint artifact if the resume path starts with WANDB_ARTIFACT_PREFIX arguments: opt (namespace) -- Commandline arguments for this run \"\"\" if opt [ \"resume\" ] . startswith ( WANDB_ARTIFACT_PREFIX ): model_artifact = wandb . use_artifact ( remove_prefix ( opt [ \"resume\" ], WANDB_ARTIFACT_PREFIX ) + \":latest\" ) assert model_artifact is not None , \"Error: W&B model artifact doesn't exist\" modeldir = model_artifact . download () # epochs_trained = model_artifact.metadata.get(\"epochs_trained\") total_epochs = model_artifact . metadata . get ( \"total_epochs\" ) is_finished = total_epochs is None assert ( not is_finished ), \"training is finished, can only resume incomplete runs.\" return modeldir , model_artifact return None , None","title":"download_model_artifact()"},{"location":"api/#gdrf.utils.wandblogger.WandbLogger.end_epoch","text":"commit the log_dict, model artifacts and Tables to W&B and flush the log_dict. best_result (boolean): Boolean representing if the result of this evaluation is best or not Source code in gdrf/utils/wandblogger.py def end_epoch ( self , best_result = False ): \"\"\" commit the log_dict, model artifacts and Tables to W&B and flush the log_dict. arguments: best_result (boolean): Boolean representing if the result of this evaluation is best or not \"\"\" if self . wandb_run : with all_logging_disabled (): wandb . log ( self . log_dict ) self . log_dict = {} if self . result_artifact : wandb . log_artifact ( self . result_artifact , aliases = [ \"latest\" , \"last\" , \"epoch \" + str ( self . current_epoch ), ( \"best\" if best_result else \"\" ), ], ) self . result_artifact = wandb . Artifact ( \"run_\" + wandb . run . id + \"_progress\" , \"evaluation\" )","title":"end_epoch()"},{"location":"api/#gdrf.utils.wandblogger.WandbLogger.finish_run","text":"Log metrics if any and finish the current W&B run Source code in gdrf/utils/wandblogger.py def finish_run ( self , ** kwargs ): \"\"\" Log metrics if any and finish the current W&B run \"\"\" if all ( kwargs . get ( x , None ) is not None for x in [ \"artifact_or_path\" , \"type\" , \"name\" , \"aliases\" ] ): wandb . log_artifact ( artifact_or_path = kwargs . get ( \"artifact_or_path\" ), type = kwargs . get ( \"type\" ), name = kwargs . get ( \"name\" ), aliases = kwargs . get ( \"aliases\" ), ) if self . wandb_run : if self . log_dict : with all_logging_disabled (): wandb . log ( self . log_dict ) wandb . run . finish ()","title":"finish_run()"},{"location":"api/#gdrf.utils.wandblogger.WandbLogger.log","text":"save the metrics to the logging dictionary log_dict (Dict) -- metrics/media to be logged in current step Source code in gdrf/utils/wandblogger.py def log ( self , log_dict ): \"\"\" save the metrics to the logging dictionary arguments: log_dict (Dict) -- metrics/media to be logged in current step \"\"\" if self . wandb_run : for key , value in log_dict . items (): self . log_dict [ key ] = value","title":"log()"},{"location":"api/#gdrf.utils.wandblogger.WandbLogger.log_dataset_artifact","text":"Log the dataset as W&B artifact and return the new data file with W&B links data_file (str) -- the .yaml file with information about the dataset like - path, classes etc. single_class (boolean) -- train multi-class data as single-class project (str) -- project name. Used to construct the artifact path overwrite_config (boolean) -- overwrites the data.yaml file if set to true otherwise creates a new file with _wandb postfix. Eg -> data_wandb.yaml the new .yaml file with artifact links. it can be used to start training directly from artifacts Source code in gdrf/utils/wandblogger.py def log_dataset_artifact ( self , data_file , project , overwrite_config = False ): \"\"\" Log the dataset as W&B artifact and return the new data file with W&B links arguments: data_file (str) -- the .yaml file with information about the dataset like - path, classes etc. single_class (boolean) -- train multi-class data as single-class project (str) -- project name. Used to construct the artifact path overwrite_config (boolean) -- overwrites the data.yaml file if set to true otherwise creates a new file with _wandb postfix. Eg -> data_wandb.yaml returns: the new .yaml file with artifact links. it can be used to start training directly from artifacts \"\"\" self . data_dict = check_dataset ( data_file ) # parse and check data = dict ( self . data_dict ) self . train_artifact = ( self . create_dataset_table ( pd . read_csv ( data [ \"train\" ], index_col = 0 ), name = \"train\" ) if data . get ( \"train\" ) else None ) if data . get ( \"train\" ): data [ \"train\" ] = WANDB_ARTIFACT_PREFIX + str ( Path ( project ) / \"train\" ) if data . get ( \"val\" ): data [ \"val\" ] = WANDB_ARTIFACT_PREFIX + str ( Path ( project ) / \"val\" ) path = Path ( data_file ) . stem path = ( path if overwrite_config else path + \"_wandb\" ) + \".csv\" # updated data.yaml path data . pop ( \"download\" , None ) data . pop ( \"path\" , None ) if self . job_type == \"Training\" : # builds correct artifact pipeline graph self . wandb_run . use_artifact ( self . train_artifact ) else : self . wandb_run . log_artifact ( self . train_artifact ) return path","title":"log_dataset_artifact()"},{"location":"api/#gdrf.utils.wandblogger.WandbLogger.log_model","text":"Log the model checkpoint as W&B artifact path (Path) -- Path of directory containing the checkpoints opt (namespace) -- Command line arguments for this run epoch (int) -- Current epoch number fitness_score (float) -- fitness score for current epoch best_model (boolean) -- Boolean representing if the current checkpoint is the best yet. Source code in gdrf/utils/wandblogger.py def log_model ( self , path , opt , epoch , fitness_score , best_model = False ): \"\"\" Log the model checkpoint as W&B artifact arguments: path (Path) -- Path of directory containing the checkpoints opt (namespace) -- Command line arguments for this run epoch (int) -- Current epoch number fitness_score (float) -- fitness score for current epoch best_model (boolean) -- Boolean representing if the current checkpoint is the best yet. \"\"\" model_artifact = wandb . Artifact ( \"run_\" + wandb . run . id + \"_model\" , type = \"model\" , metadata = { \"original_url\" : str ( path ), \"epochs_trained\" : epoch + 1 , \"save period\" : opt [ \"save_period\" ], \"project\" : opt [ \"project\" ], \"total_epochs\" : opt [ \"epochs\" ], \"fitness_score\" : fitness_score , }, ) model_artifact . add_file ( str ( path / \"last.pt\" ), name = \"last.pt\" ) wandb . log_artifact ( model_artifact , aliases = [ \"latest\" , \"last\" , \"epoch \" + str ( self . current_epoch ), \"best\" if best_model else \"\" , ], ) print ( \"Saving model artifact on epoch \" , epoch + 1 )","title":"log_model()"},{"location":"api/#gdrf.utils.wandblogger.WandbLogger.setup_training","text":"Setup the necessary processes for training YOLO models: - Attempt to download model checkpoint and dataset artifacts if opt.resume stats with WANDB_ARTIFACT_PREFIX - Update data_dict, to contain info of previous run if resumed and the paths of dataset artifact if downloaded - Setup log_dict, initialize bbox_interval opt (namespace) -- commandline arguments for this run Source code in gdrf/utils/wandblogger.py def setup_training ( self , opt ): \"\"\" Setup the necessary processes for training YOLO models: - Attempt to download model checkpoint and dataset artifacts if opt.resume stats with WANDB_ARTIFACT_PREFIX - Update data_dict, to contain info of previous run if resumed and the paths of dataset artifact if downloaded - Setup log_dict, initialize bbox_interval arguments: opt (namespace) -- commandline arguments for this run \"\"\" self . log_dict , self . current_epoch = {}, 0 if isinstance ( opt [ \"resume\" ], str ): modeldir , _ = self . download_model_artifact ( opt ) if modeldir : self . weights = Path ( modeldir ) / \"last.pt\" config = self . wandb_run . config for k , v in config . items (): if k in opt : if k in self . opt_transforms : v = self . opt_transforms [ k ]( v ) opt [ k ] = v data_dict = self . data_dict self . train_artifact_path , self . train_artifact = self . download_dataset_artifact ( data_dict . get ( \"train\" ), opt [ \"artifact_alias\" ] ) if self . train_artifact_path is not None : data_dict [ \"train\" ] = str ( Path ( self . train_artifact_path )) train_from_artifact = self . train_artifact_path is not None # Update the the data_dict to point to local artifacts dir if train_from_artifact : self . data_dict = data_dict","title":"setup_training()"},{"location":"api/#gdrf.utils.wandblogger.all_logging_disabled","text":"source - https://gist.github.com/simon-weber/7853144 A context manager that will prevent any logging messages triggered during the body from being processed. :param highest_level: the maximum logging level in use. This would only need to be changed if a custom level greater than CRITICAL is defined. Source code in gdrf/utils/wandblogger.py @contextmanager def all_logging_disabled ( highest_level = logging . CRITICAL ): \"\"\"source - https://gist.github.com/simon-weber/7853144 A context manager that will prevent any logging messages triggered during the body from being processed. :param highest_level: the maximum logging level in use. This would only need to be changed if a custom level greater than CRITICAL is defined. \"\"\" previous_level = logging . root . manager . disable logging . disable ( highest_level ) try : yield finally : logging . disable ( previous_level )","title":"all_logging_disabled()"},{"location":"api/#gdrf.visualize","text":"","title":"visualize"},{"location":"api/#gdrf.visualize.matrixplot_cli","text":"Creates a matrix plot from the probabilities in the CSV file data . The first row of the CSV file should be a header. The first column of the CSV file should be an index. All other columns are considered data. Optionally, the probabilities may be plotted on a log-scale colorbar. :param str data: A CSV file with probabilities. Header row is required, index column is required. :param bool log: An optional CSV file with the index. Header row is required. :param float epsilon: An optional \"jitter\" parameter, for plotting small probabilities on a log-scale Source code in gdrf/visualize/__init__.py def matrixplot_cli ( data : str , log : bool = False , epsilon : float = 1e-10 ): \"\"\" Creates a matrix plot from the probabilities in the CSV file `data`. The first row of the CSV file should be a header. The first column of the CSV file should be an index. All other columns are considered data. Optionally, the probabilities may be plotted on a log-scale colorbar. :param str data: A CSV file with probabilities. Header row is required, index column is required. :param bool log: An optional CSV file with the index. Header row is required. :param float epsilon: An optional \"jitter\" parameter, for plotting small probabilities on a log-scale \"\"\" display ( matrix_plot ( data , log = log , epsilon = epsilon ))","title":"matrixplot_cli()"},{"location":"api/#gdrf.visualize.maxplot_2d_cli","text":"Creates a 2-D maximum plot from the probabilities in the CSV file data . Optionally, a two-column CSV file index may be provided. The first row of CSV files should be a header. All columns are considered data, except if no index CSV is provided the first two columns of data are the index. :param str data: A CSV file with probabilities. Header row is required, index columns are optional :param str index: An optional CSV file with the index. Header row is required. Source code in gdrf/visualize/__init__.py def maxplot_2d_cli ( data : str , index : str = None ): \"\"\" Creates a 2-D maximum plot from the probabilities in the CSV file `data`. Optionally, a two-column CSV file `index` may be provided. The first row of CSV files should be a header. All columns are considered data, except if no `index` CSV is provided the first two columns of `data` are the index. :param str data: A CSV file with probabilities. Header row is required, index columns are optional :param str index: An optional CSV file with the index. Header row is required. \"\"\" display ( maxplot_2d ( data , index ))","title":"maxplot_2d_cli()"},{"location":"api/#gdrf.visualize.stackplot_1d_cli","text":"Creates a 1-D stackplot from the probabilities in the CSV file data . Optionally, a single-column CSV file index may be provided. The first row of CSV files should be a header. All columns are considered data, except if no index CSV is provided the first column of data is the index. :param str data: A CSV file with probabilities. Header row is required, index column is optional :param str index: An optional CSV file with the index. Header row is required. Source code in gdrf/visualize/__init__.py def stackplot_1d_cli ( data : str , index : str = None ): \"\"\" Creates a 1-D stackplot from the probabilities in the CSV file `data`. Optionally, a single-column CSV file `index` may be provided. The first row of CSV files should be a header. All columns are considered data, except if no `index` CSV is provided the first column of `data` is the index. :param str data: A CSV file with probabilities. Header row is required, index column is optional :param str index: An optional CSV file with the index. Header row is required. \"\"\" display ( stackplot_1d ( data , index ))","title":"stackplot_1d_cli()"},{"location":"authors/","text":"Credits \u00b6 Development Lead \u00b6 John San Soucie jsansoucie@whoi.edu Contributors \u00b6 None yet. Why not be the first?","title":"authors"},{"location":"authors/#credits","text":"","title":"Credits"},{"location":"authors/#development-lead","text":"John San Soucie jsansoucie@whoi.edu","title":"Development Lead"},{"location":"authors/#contributors","text":"None yet. Why not be the first?","title":"Contributors"},{"location":"contributing/","text":"Contributing \u00b6 Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways: Types of Contributions \u00b6 Report Bugs \u00b6 Report bugs at https://github.com/san-soucie/gdrf/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug. Fix Bugs \u00b6 Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it. Implement Features \u00b6 Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it. Write Documentation \u00b6 Gaussian-Dirichlet Random Fields could always use more documentation, whether as part of the official Gaussian-Dirichlet Random Fields docs, in docstrings, or even on the web in blog posts, articles, and such. Submit Feedback \u00b6 The best way to send feedback is to file an issue at https://github.com/san-soucie/gdrf/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :) Get Started! \u00b6 Ready to contribute? Here's how to set up gdrf for local development. Fork the gdrf repo on GitHub. Clone your fork locally 1 $ git clone git@github.com:your_name_here/gdrf.git Ensure poetry is installed. Install dependencies and start your virtualenv: 1 $ poetry install -E test -E doc -E dev Create a branch for local development: 1 $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: 1 $ tox Commit your changes and push your branch to GitHub: 1 2 3 $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website. Pull Request Guidelines \u00b6 Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for Python 3.6, 3.7, 3.8, 3.9 and for PyPy. Check https://github.com/san-soucie/gdrf/actions and make sure that the tests pass for all supported Python versions. Tips``` \u00b6 1 $ pytest tests.test_gdrf ```To run a subset of tests. Deploying \u00b6 A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in HISTORY.md). Then run: 1 2 3 $ poetry patch # possible: major / minor / patch $ git push $ git push --tags Travis will then deploy to PyPI if tests pass.","title":"contributing"},{"location":"contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways:","title":"Contributing"},{"location":"contributing/#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"contributing/#report-bugs","text":"Report bugs at https://github.com/san-soucie/gdrf/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug.","title":"Report Bugs"},{"location":"contributing/#fix-bugs","text":"Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.","title":"Fix Bugs"},{"location":"contributing/#implement-features","text":"Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.","title":"Implement Features"},{"location":"contributing/#write-documentation","text":"Gaussian-Dirichlet Random Fields could always use more documentation, whether as part of the official Gaussian-Dirichlet Random Fields docs, in docstrings, or even on the web in blog posts, articles, and such.","title":"Write Documentation"},{"location":"contributing/#submit-feedback","text":"The best way to send feedback is to file an issue at https://github.com/san-soucie/gdrf/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :)","title":"Submit Feedback"},{"location":"contributing/#get-started","text":"Ready to contribute? Here's how to set up gdrf for local development. Fork the gdrf repo on GitHub. Clone your fork locally 1 $ git clone git@github.com:your_name_here/gdrf.git Ensure poetry is installed. Install dependencies and start your virtualenv: 1 $ poetry install -E test -E doc -E dev Create a branch for local development: 1 $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: 1 $ tox Commit your changes and push your branch to GitHub: 1 2 3 $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website.","title":"Get Started!"},{"location":"contributing/#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for Python 3.6, 3.7, 3.8, 3.9 and for PyPy. Check https://github.com/san-soucie/gdrf/actions and make sure that the tests pass for all supported Python versions.","title":"Pull Request Guidelines"},{"location":"contributing/#tips","text":"1 $ pytest tests.test_gdrf ```To run a subset of tests.","title":"Tips```"},{"location":"contributing/#deploying","text":"A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in HISTORY.md). Then run: 1 2 3 $ poetry patch # possible: major / minor / patch $ git push $ git push --tags Travis will then deploy to PyPI if tests pass.","title":"Deploying"},{"location":"history/","text":"History \u00b6 0.1.0 (2021-07-02) \u00b6 First release on PyPI.","title":"history"},{"location":"history/#history","text":"","title":"History"},{"location":"history/#010-2021-07-02","text":"First release on PyPI.","title":"0.1.0 (2021-07-02)"},{"location":"installation/","text":"Installation \u00b6 Stable release \u00b6 To install Gaussian-Dirichlet Random Fields, run this command in your terminal: 1 $ pip install gdrf This is the preferred method to install Gaussian-Dirichlet Random Fields, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process. From source \u00b6 The source for Gaussian-Dirichlet Random Fields can be downloaded from the Github repo . You can either clone the public repository: 1 $ git clone git://github.com/san-soucie/gdrf Or download the tarball : 1 $ curl -OJL https://github.com/san-soucie/gdrf/tarball/master Once you have a copy of the source, you can install it with: 1 $ pip install .","title":"installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#stable-release","text":"To install Gaussian-Dirichlet Random Fields, run this command in your terminal: 1 $ pip install gdrf This is the preferred method to install Gaussian-Dirichlet Random Fields, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process.","title":"Stable release"},{"location":"installation/#from-source","text":"The source for Gaussian-Dirichlet Random Fields can be downloaded from the Github repo . You can either clone the public repository: 1 $ git clone git://github.com/san-soucie/gdrf Or download the tarball : 1 $ curl -OJL https://github.com/san-soucie/gdrf/tarball/master Once you have a copy of the source, you can install it with: 1 $ pip install .","title":"From source"},{"location":"usage/","text":"Usage \u00b6 To use Gaussian-Dirichlet Random Fields in a project 1 import gdrf","title":"usage"},{"location":"usage/#usage","text":"To use Gaussian-Dirichlet Random Fields in a project 1 import gdrf","title":"Usage"}]}